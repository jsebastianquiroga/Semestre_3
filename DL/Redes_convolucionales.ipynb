{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m4cmOojch_to",
        "gKiIShYiXv1C",
        "ydlq9_vRaUdQ",
        "yfxtbQ_2ZrWL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsebastianquiroga/analitica_ia_puj/blob/main/DL/Redes_convolucionales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes Recurrentes\n",
        "\n",
        "\n",
        "Las redes convolucionales son un tipo de arquitectura de redes neuronales artificiales que han demostrado ser muy efectivas en el procesamiento y clasificación de imágenes. Utilizan filtros convolucionales que escanean la imagen pixel a pixel, capturando patrones en diferentes niveles de detalle. Estos filtros se combinan en capas convolucionales para extraer características cada vez más abstractas de la imagen.\n",
        "\n",
        "Además de las capas convolucionales, las redes convolucionales también incluyen capas de pooling para reducir la dimensión de la imagen y capas totalmente conectadas para clasificar las características extraídas en categorías específicas.\n",
        "\n",
        "Las redes convolucionales han demostrado ser muy efectivas en una amplia gama de aplicaciones de visión por computadora, incluyendo reconocimiento de objetos, detección de rostros, segmentación de imágenes y más. También se han utilizado con éxito en otros campos, como el procesamiento de señales y el procesamiento del lenguaje natural."
      ],
      "metadata": {
        "id": "ClYkxUfMBone"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLAMAN LAS LIBRERIAS NECESARIAS\n",
        "\n",
        "import os\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage import color\n",
        "from skimage import io\n",
        "\n",
        "from matplotlib.image import imread\n",
        "\n",
        "# sE CONECTA CON DRIVE PARA DESCARGAR IMAGEN DE 7X7X3\n",
        "\n",
        "# Conexion a gdrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from google.colab import drive\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "\n",
        "from oauth2client.client import GoogleCredentials \n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Instalar la biblioteca Pillow\n",
        "!pip install pillow\n",
        "\n",
        "# Importar bibliotecas necesarias\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Descargar la imagen desde Google Drive\n",
        "image_id = '1syXUIzjd0eJft4zKUKVebDvYS3PK5_7-'\n",
        "download = drive.CreateFile({'id': image_id})\n",
        "download.GetContentFile('cuadrado7x7.jpg') # Guarda la imagen en un archivo temporal\n",
        "\n",
        "# Leer la imagen y visualizarla\n",
        "image = Image.open('cuadrado7x7.jpg')\n",
        "image.show()\n"
      ],
      "metadata": {
        "id": "eNwMj9hG_NUS",
        "outputId": "bbcf9dc8-a218-4f2a-9423-be9898726c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=7x7 at 0x7FA704436C20>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAcAAAAHCAIAAABLMMCEAAAAJklEQVR4nGP8//8/AwZgwhRiYGBggVCMjIwQBkQrdrXYRRlJsA0AYIIJCWiXPBYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# se convierte en numpy para traer la tercera dimencion, la escala de color.\n",
        "imagen = np.array(image)\n",
        "# plot inicial\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.title('imagen 1')\n",
        "plt.imshow(imagen)"
      ],
      "metadata": {
        "id": "v6xl-rijBIO3",
        "outputId": "98ae9351-1519-48d4-d040-e73ffdf33e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa704436cb0>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Se pasa a escala de grises para quitar tercera dimencion, queda 7X7X1\n",
        "imagen_2 = color.rgb2gray(imagen)\n",
        "imagenBN = imagen_2\n",
        "print('Dimenciones matriz :', imagenBN.shape)\n",
        "imagenBN"
      ],
      "metadata": {
        "id": "rle54FN5BhzD",
        "outputId": "1a0be02f-b42d-44b4-9420-28899c011a34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimenciones matriz : (7, 7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 0., 0., 0., 1., 1.],\n",
              "       [1., 1., 0., 0., 0., 1., 1.],\n",
              "       [1., 1., 0., 0., 0., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.title('imagen 1')\n",
        "plt.imshow(imagenBN)"
      ],
      "metadata": {
        "id": "YkjIlczTDtMI",
        "outputId": "901fe791-96fc-4944-ee38-d84c88c3d048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa70536c490>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea un kernel para demostrar el paso de la convolución."
      ],
      "metadata": {
        "id": "6BQu3yX1D0II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filtro de convolución 2D también conocido como filtro Laplaciano. \n",
        "#Un filtro Laplaciano se utiliza para realzar los bordes y los detalles de una imagen. \n",
        " \n",
        "kernel = np.array((\n",
        "    [ 0, -1,  0],\n",
        "    [-1,  4, -1],\n",
        "    [ 0, -1,  0]\n",
        "    ), dtype = 'float')"
      ],
      "metadata": {
        "id": "76TU3G3cDzpq"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proceso convolución"
      ],
      "metadata": {
        "id": "WD0s0ExREmlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar la convolución 2D de la imagen en escala de grises (imagenBN) con un kernel dado\n",
        "conv = signal.convolve2d(imagenBN, kernel, boundary='symm', mode='same')\n",
        "\n",
        "# Crear una nueva figura de tamaño 5x5 (pulgadas)\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "# Establecer el título de la figura como \"Filtro H4\"\n",
        "plt.title(\"Filtro H4\")\n",
        "\n",
        "# Desactivar los ejes para obtener una visualización limpia de la imagen\n",
        "plt.axis('off')\n",
        "\n",
        "# Mostrar la imagen convolucionada en escala de grises\n",
        "plt.imshow(conv, cmap='gray')"
      ],
      "metadata": {
        "id": "nN3PxxlTDwmN",
        "outputId": "d947896c-e551-4ae8-9293-179f3dc79f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa70529de70>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes Convolucionales predicción Perros y Gatos\n",
        "\n",
        "Este código es un ejemplo de cómo se puede utilizar una red neuronal convolucional para la clasificación de imágenes de perros y gatos. Se utiliza la librería Keras para definir la arquitectura de la red y entrenarla.\n",
        "\n",
        "El proceso comienza montando Google Drive, desde donde se va a descargar un archivo comprimido con las imágenes de entrenamiento y validación, en carpetas separadas de perros y gatos. Se extraen las imágenes y se crea un objeto ImageDataGenerator que se encargará de cargar las imágenes a medida que el modelo las necesite. Además, se crea la estructura de la red convolucional.\n",
        "\n",
        "La red convolucional consta de varias capas que procesan las imágenes en diferentes formas. Primero se tienen las capas de convolución, que aplican un conjunto de filtros a la imagen para resaltar características específicas, como bordes o patrones. Luego, se utiliza una capa de pooling para reducir el tamaño de la imagen y disminuir la cantidad de parámetros que deben ser aprendidos. Después de varias capas de convolución y pooling, se utiliza una capa de flatten para aplanar la imagen en un vector unidimensional. Por último, se utilizan capas densas para realizar la clasificación.\n",
        "\n",
        "El modelo se entrena con las imágenes de entrenamiento y se valida con las imágenes de validación. Después de entrenar el modelo, se pueden hacer predicciones con nuevas imágenes de perros o gatos utilizando la función predict."
      ],
      "metadata": {
        "id": "EqZcmpsUpDWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las liobrerías y paquetes\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "CRQyx2kfKyZ0"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar la imagen desde Google Drive\n",
        "folder = '1PlF-DNvFETknsJu5XUpRxFnK8bKeSG0P'\n",
        "download = drive.CreateFile({'id': folder})\n",
        "download.GetContentFile('dataset.zip') # Guarda la imagen en un archivo temporal\n",
        "\n",
        "!unzip -q dataset.zip | head -n 5"
      ],
      "metadata": {
        "id": "zv6spHEGaao7",
        "outputId": "5743d9ee-f3bd-4583-d797-c7d7beb5becc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace dataset/single_prediction/cat_or_dog_3.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/dataset'\n",
        "\n",
        "print(os.listdir(PATH))\n",
        "\n",
        "train_dir = os.path.join(PATH,\n",
        "                         'training_set')\n",
        "test_dir = os.path.join(PATH,\n",
        "                        'test_set')\n",
        "validation_dir = os.path.join(PATH,\n",
        "                         'validation_set')\n",
        "\n",
        "# train_cats_dir = os.path.join(train_dir, 'cats') # train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "# test_cats_dir = os.path.join(test_dir, 'cats') #  test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
        "# validation_cats_dir = os.path.join(validation_dir, 'cats')  #  validation_dogs_dir = os.path.join(validation_dir,'dogs')"
      ],
      "metadata": {
        "id": "79qhF3j2d8mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "\n",
        "#valores  de entrada:\n",
        "\n",
        "pixel=150 #imagenes de 150 pixeles\n",
        "size=20\n",
        "epocas=3\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "metadata": {
        "id": "YE7YLlJ-K58q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(pixel, pixel),\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "qjx2WZZPKm-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(pixel, pixel),      #reescalado\n",
        "    color_mode=\"grayscale\",          #rgb color, grayscale grises   \n",
        "    batch_size=size,             #num de imagenes producidas por el generador de lotes \n",
        "    class_mode='binary',       #binary para 2 clases, categorical para mas de 2 clases\n",
        "    shuffle=True,              #mezclar el orden de la imagen que se está produciendo\n",
        "    seed=42                    #semilla aleatoria\n",
        ")"
      ],
      "metadata": {
        "id": "KzzH89aMML3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(pixel, pixel),      #reescalado\n",
        "    color_mode=\"grayscale\",          #rgb color, grayscale grises   \n",
        "    batch_size=size,             #num de imagenes producidas por el generador de lotes \n",
        "    class_mode='binary',       #binary para 2 clases, categorical para mas de 2 clases\n",
        "    shuffle=True,              #mezclar el orden de la imagen que se está produciendo\n",
        "    seed=42                    #semilla aleatoria\n",
        ")"
      ],
      "metadata": {
        "id": "qoqj6xpmMS6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una red convolucional (Convolutional Neural Network o CNN) es un tipo de red neuronal profunda que se utiliza principalmente para tareas de clasificación y reconocimiento de imágenes.\n",
        "\n",
        "La clase Sequential de Keras es una forma sencilla de crear una red neuronal apilando capas de manera secuencial. En el caso de una red convolucional, estas capas suelen ser capas convolucionales, de agrupación (pooling) y de activación.\n",
        "\n",
        "Aquí te muestro un ejemplo de cómo se podría definir una red convolucional en Keras utilizando Sequential y agregarle una capa de clasificación (fully-connected) al final:"
      ],
      "metadata": {
        "id": "jxXYkt2giGfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = Sequential()"
      ],
      "metadata": {
        "id": "BVwZuIU0h5gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 1: Convolución\n",
        "\n",
        "\n",
        "\n",
        "Esta es una capa de convolución en una red neuronal convolucional (CNN). A continuación se explican los argumentos utilizados:\n",
        "\n",
        "filters: número de filtros (también conocidos como kernels) que se aplicarán a la imagen de entrada para producir las características en la siguiente capa. En este caso, se utilizan 32 filtros.\n",
        "\n",
        "padding: especifica cómo se debe rellenar la imagen de entrada en el borde para que la convolución no reduzca la resolución de la imagen de salida. En este caso, se utiliza same para que el tamaño de la salida sea el mismo que el tamaño de la entrada.\n",
        "\n",
        "kernel_size: tamaño del filtro. En este caso, se utiliza un filtro de 3x3.\n",
        "\n",
        "use_bias: indica si se utilizará un sesgo (bias) en la convolución. En este caso, se establece en 1 para usar un sesgo.\n",
        "\n",
        "input_shape: forma de la imagen de entrada. En este caso, la imagen de entrada tiene una forma de (pixel, pixel, 1), donde pixel se refiere al tamaño de la imagen y 1 indica que es una imagen en escala de grises.\n",
        "\n",
        "activation: función de activación que se aplicará después de la convolución. En este caso, se utiliza la función de activación relu.\n",
        "\n",
        "En resumen, esta capa de convolución aplicará 32 filtros de 3x3 a una imagen de entrada en escala de grises de tamaño (pixel, pixel) y aplicará una función de activación ReLU a la salida."
      ],
      "metadata": {
        "id": "mAFpsMwnizAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolución\n",
        "capa11=Conv2D(filters = 32,\n",
        "              padding='same',\n",
        "              kernel_size = (3, 3),\n",
        "              use_bias=1,\n",
        "              input_shape = (pixel, pixel, 1), \n",
        "              activation = \"relu\")"
      ],
      "metadata": {
        "id": "trMhF62gi43I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#paso 2: max Pooling\n",
        "\n",
        "MaxPooling2D y Dropout son capas comunes en las redes convolucionales:\n",
        "\n",
        "MaxPooling2D es una técnica para reducir el tamaño de los mapas de características al seleccionar el valor máximo dentro de una ventana de tamaño determinado y deslizarla por toda la imagen. Esta técnica se utiliza para disminuir el tamaño de la entrada y así reducir la complejidad del modelo y disminuir el overfitting.\n",
        "\n",
        "Dropout es una técnica para regularizar la red neuronal y prevenir el overfitting. Durante el entrenamiento, la capa Dropout selecciona aleatoriamente algunas neuronas y las apaga (es decir, establece sus valores a cero). Esto hace que otras neuronas se activen y se fortalezcan para compensar la pérdida, lo que reduce la dependencia de ciertas neuronas y hace que la red sea más robusta y generalizable.\n",
        "\n",
        "En el código que has proporcionado, capa12 es una capa de pooling que reduce el tamaño de la imagen de entrada y capa13 es una capa Dropout con una tasa de 0.25, lo que significa que se apagan aleatoriamente el 25% de las neuronas durante el entrenamiento."
      ],
      "metadata": {
        "id": "YTX_LShQjTBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capa12=MaxPooling2D(pool_size = (2,2))\n",
        "capa13=Dropout(0.25)"
      ],
      "metadata": {
        "id": "ThxStom-jTXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capa 2\n",
        "\n",
        "\n",
        "Al igual que la capa anterior, está configurada para tener 64 filtros, cada uno con una dimensión de 3x3. La opción padding='same' significa que se rellena con ceros alrededor del borde de la imagen de entrada para que la salida tenga las mismas dimensiones que la entrada. La opción use_bias=1 significa que se utilizan los sesgos. Y la función de activación utilizada en esta capa es \"relu\", que es una función no lineal comúnmente utilizada en redes neuronales convolucionales."
      ],
      "metadata": {
        "id": "q6xCWevTjb7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capa21=Conv2D(filters = 64,\n",
        "             padding='same',\n",
        "             kernel_size = (3, 3),\n",
        "             use_bias=1,\n",
        "             activation = \"relu\")"
      ],
      "metadata": {
        "id": "FklrH4Apjmc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capa22=MaxPooling2D(pool_size = (2,2))\n",
        "capa23=Dropout(0.25)"
      ],
      "metadata": {
        "id": "oER_Wgz4jrZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capa 3\n",
        "\n",
        "\n",
        "\n",
        "Esta línea de código define la tercera capa de convolución en la red neuronal convolucional. La capa se llama capa31 y es un objeto de la clase Conv2D que realiza la convolución de una imagen de entrada con 128 filtros de 3x3 píxeles cada uno.\n",
        "\n",
        "Los parámetros de la capa son:\n",
        "\n",
        "filters: número de filtros que se utilizan en la convolución.\n",
        "padding: tipo de relleno que se aplica en los bordes de la imagen de entrada. En este caso, padding='same' indica que se realiza un relleno para que la salida tenga el mismo tamaño que la imagen de entrada.\n",
        "kernel_size: tamaño del kernel o filtro de convolución. En este caso, se utiliza un kernel de 3x3 píxeles.\n",
        "use_bias: especifica si se utiliza un término de sesgo en la operación de convolución. En este caso, se utiliza un término de sesgo con use_bias=1.\n",
        "activation: función de activación que se aplica a la salida de la capa. En este caso, se utiliza la función de activación ReLU (activation=\"relu\") para introducir no-linealidad en la red."
      ],
      "metadata": {
        "id": "W9SIELeNkTY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capa31=Conv2D(filters = 128,\n",
        "             padding='same',\n",
        "             kernel_size = (3, 3),\n",
        "             use_bias=1,\n",
        "             activation = \"relu\")"
      ],
      "metadata": {
        "id": "7uwl4b77kUrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capa32=MaxPooling2D(pool_size = (2,2))\n",
        "capa33=Dropout(0.25)"
      ],
      "metadata": {
        "id": "5U8OE-nvkXoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capa 4\n",
        "\n",
        "Esta es otra capa de convolución que tiene 128 filtros (también conocidos como kernels o detectores de características) y un tamaño de kernel de (3, 3), lo que significa que cada filtro tendrá una ventana deslizante de 3x3 píxeles en la imagen de entrada. El padding es \"same\", lo que significa que la salida tendrá la misma forma que la entrada. El parámetro use_bias se establece en 1 para incluir sesgos en la capa. Finalmente, la función de activación \"relu\" se utiliza para introducir no linealidad en la capa."
      ],
      "metadata": {
        "id": "s6h9YsrGkdDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capa41=Conv2D(filters = 128,\n",
        "             padding='same',\n",
        "             kernel_size = (3, 3),\n",
        "             use_bias=1,\n",
        "             activation = \"relu\")"
      ],
      "metadata": {
        "id": "wBPuwruxkZCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capa42=MaxPooling2D(pool_size = (2,2))\n",
        "capa43=Dropout(0.25)"
      ],
      "metadata": {
        "id": "ozRkChUzki-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La capa Flatten() se utiliza para aplanar el tensor de características (feature map) obtenido a partir de las capas convolucionales y de pooling, para que pueda ser procesado por las capas densas (fully connected layers).\n",
        "\n",
        "El tensor de características es una matriz multidimensional que contiene la información de las características extraídas de las imágenes en cada una de las capas convolucionales y de pooling. La capa Flatten() se encarga de \"aplanar\" este tensor para que pueda ser pasado como entrada a las capas densas.\n",
        "\n",
        "Por ejemplo, si tenemos un tensor de características de dimensiones 9x9x128 (9 filas, 9 columnas y 128 canales), la capa Flatten() lo transformará en un vector de 6272 elementos (9x9x128=10.368), que será la entrada de las capas densas."
      ],
      "metadata": {
        "id": "MoCOZ4xpkrM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capa51=Flatten()"
      ],
      "metadata": {
        "id": "bgt68LP0kuEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas dos capas corresponden a la parte densa de la red neuronal, donde las características extraídas por las capas convolucionales se \"aplanan\" y se alimentan a una red de neuronas totalmente conectadas.\n",
        "\n",
        "La primera capa densa tiene 512 neuronas y utiliza la función de activación ReLU (rectified linear unit), que es una función no lineal que asigna valores negativos a cero y los valores positivos los mantiene igual. Esta capa ayuda a extraer características más complejas de las imágenes y aporta robustez al modelo.\n",
        "\n",
        "La segunda capa densa tiene una sola neurona y utiliza la función de activación sigmoide, que produce una salida en el rango de 0 a 1, que representa la probabilidad de que la imagen de entrada pertenezca a la clase positiva (perro) o a la clase negativa (gato). Esta capa es la capa de salida del modelo."
      ],
      "metadata": {
        "id": "ltHYcVOXlPq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capa61=Dense(512, use_bias=1, activation = \"relu\")\n",
        "capa62=Dense(1, use_bias=1, activation = \"sigmoid\")"
      ],
      "metadata": {
        "id": "XeZ4Md3HlRBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Armado\n",
        "\n",
        "classifier.add(capa11)\n",
        "classifier.add(capa12)\n",
        "\n",
        "classifier.add(capa21)\n",
        "classifier.add(capa22)\n",
        "\n",
        "classifier.add(capa31)\n",
        "classifier.add(capa32)\n",
        "\n",
        "classifier.add(capa41)\n",
        "classifier.add(capa42)\n",
        "\n",
        "classifier.add(capa51)\n",
        "\n",
        "classifier.add(capa61)\n",
        "classifier.add(capa62)\n",
        "\n",
        "classifier.compile(loss = \"binary_crossentropy\",\n",
        "                   optimizer = \"adam\",\n",
        "                   #optimizer = \"sgd\",\n",
        "                   #optimizer = \"rmsprop\",\n",
        "                   metrics=['acc', 'mse'])\n",
        "\n",
        "classifier.summary()"
      ],
      "metadata": {
        "id": "_erv38d6lSTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_generator.n devuelve el número total de imágenes en el conjunto de datos de entrenamiento, y train_generator.batch_size indica el tamaño del lote utilizado para el entrenamiento. Dividiendo el número total de imágenes por el tamaño del lote, se obtiene el número de pasos necesarios para recorrer todo el conjunto de datos. Esto se hace para que la red neuronal pueda entrenar en varias etapas en lugar de una sola etapa, lo que ayuda a evitar problemas de memoria.\n",
        "\n",
        "Lo mismo se aplica para el conjunto de datos de validación: validation_generator.n devuelve el número total de imágenes en el conjunto de datos de validación, y validation_generator.batch_size indica el tamaño del lote utilizado para la validación. Dividiendo el número total de imágenes por el tamaño del lote, se obtiene el número de pasos necesarios para recorrer todo el conjunto de datos de validación."
      ],
      "metadata": {
        "id": "GSeNOk7cmeF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pasos_entrenamiento=train_generator.n//train_generator.batch_size\n",
        "#pasos_entrenamiento=1\n",
        "pasos_validacion=validation_generator.n//validation_generator.batch_size\n",
        "#pasos_validacion=1"
      ],
      "metadata": {
        "id": "TCDcvw7GlYGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí se utiliza el método fit de Keras para entrenar el modelo. Los parámetros que se están utilizando son los siguientes:\n",
        "<ol><il> \n",
        "<li>train_generator: Es el conjunto de datos de entrenamiento, que se generó previamente utilizando el generador de imágenes. Es una instancia de la clase ImageDataGenerator.</br></li>\n",
        "<li>epochs: Es el número de épocas que se utilizarán para entrenar el modelo. Una época es una pasada completa por todo el conjunto de datos.</br></li>\n",
        "<li>steps_per_epoch: Es el número de pasos que se toman en cada época del conjunto de entrenamiento. Un paso es una actualización de los pesos del modelo. Este valor se calcula como el número de muestras de entrenamiento dividido por el tamaño del lote de entrenamiento (batch_size).</br></li>\n",
        "<li>validation_data: Es el conjunto de datos de validación, que también se generó previamente utilizando el generador de imágenes. Es una instancia de la clase ImageDataGenerator.</br></li>\n",
        "<li>validation_steps: Es similar a steps_per_epoch, pero se utiliza para el conjunto de validación.</br></li>\n",
        "<li>shuffle: Este parámetro determina si las muestras se barajan antes de cada época de entrenamiento.</br></li> </li> </ol>\n",
        "\n",
        "En resumen, esta parte del código entrena el modelo de redes neuronales convolucionales utilizando los datos de entrenamiento y validación, y devuelve un objeto History que contiene información sobre el proceso de entrenamiento."
      ],
      "metadata": {
        "id": "r65QCaq1mhbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\"\"\"\n",
        "cnn=classifier.fit(train_generator, #conjunto de entrenamiento\n",
        "               #y_test se toma de las etiquetas de los datos automaticamente\n",
        "               #https://stackoverflow.com/questions/62116637/y-argument-is-not-supported-when-using-keras-utils-sequence-as-input-error\n",
        "               #validation_generator, #conjunto de test\n",
        "               epochs=epocas, #Cuantas epocas usaremos para entrenar\n",
        "               steps_per_epoch=pasos_entrenamiento,\n",
        "               validation_data=(validation_generator),\n",
        "               validation_steps=pasos_validacion,\n",
        "               shuffle=True\n",
        "              ) \n",
        "#\"\"\""
      ],
      "metadata": {
        "id": "hX-XTFozmZOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se  carga una imagen de prueba en escala de grises usando load_img() de Keras. Luego convierte la imagen en un array usando img_to_array(). Después, agrega una dimensión adicional al array con expand_dims(), de manera que el array tenga la forma (1, pixel, pixel, 1) donde pixel es el tamaño de la imagen. Esto se debe a que predict() espera una lista de imágenes en el formato (batch_size, height, width, channels). Luego, predict() se usa para predecir si la imagen es de un perro o un gato. Finalmente, se imprime la clase predicha usando un condicional if-else."
      ],
      "metadata": {
        "id": "Cc-40IxwoMxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras.utils as image\n",
        "import os\n",
        "\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (pixel, pixel),color_mode='grayscale')\n",
        "\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = classifier.predict(test_image)\n",
        "train_generator.class_indices\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'\n",
        "\n",
        "\n",
        "predictions = classifier.predict(validation_generator)\n",
        "y_pred_df = pd.DataFrame(predictions)\n",
        "y_pred_df=np.round(y_pred_df)\n",
        "\n",
        "\n",
        "true_classes = validation_generator.classes\n",
        "y_test_df = pd.DataFrame(true_classes)\n",
        "\n",
        "\n",
        "class_labels = list(validation_generator.class_indices.keys())   \n",
        "class_labels"
      ],
      "metadata": {
        "id": "oRdBVcsioMac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Este código evalúa el modelo entrenado utilizando el conjunto de datos de entrenamiento (train_generator). La función evaluate() de Keras devuelve la pérdida y la métrica (precisión en este caso) del modelo en el conjunto de datos de entrenamiento.\n",
        "\n",
        "En este caso, steps es el número de pasos que se utilizan para evaluar el modelo, y se calcula dividiendo el número total de imágenes en el conjunto de datos de entrenamiento (train_generator.n) entre el tamaño del lote (train_generator.batch_size).\n",
        "\n",
        "Finalmente, el código imprime la precisión del modelo en el conjunto de datos de entrenamiento multiplicando el valor de la métrica (precisión) por 100."
      ],
      "metadata": {
        "id": "_HHveDq6o-Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "scores = classifier.evaluate(train_generator, steps=pasos_entrenamiento)\n",
        "print('TRAIN: ',\"\\n%s: %.2f%%\" % (classifier.metrics_names[1], scores[1]*100))\n",
        "\n",
        "#Test\n",
        "scores = classifier.evaluate(test_generator,steps=pasos_validacion)\n",
        "print('TEST: ',\"\\n%s: %.2f%%\" % (classifier.metrics_names[1], scores[1]*100))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(y_pred_df, y_test_df, target_names=class_labels)\n",
        "print(report)   "
      ],
      "metadata": {
        "id": "wgqRgcaInRUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm= confusion_matrix(y_pred_df, y_test_df)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "ax = sns.heatmap(cm/np.sum(cm), annot=True, \n",
        "            fmt='.2%', cmap='Blues')\n",
        "\n",
        "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['cat','dog'])\n",
        "ax.yaxis.set_ticklabels(['cat','dog'])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D8UiC3Kro2Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(0)  \n",
        "plt.plot(cnn.history['acc'],'r')  \n",
        "plt.plot(cnn.history['val_acc'],'g')  \n",
        "plt.xticks(np.arange(0, 2, 100))  \n",
        "plt.rcParams['figure.figsize'] = (8, 6)  \n",
        "plt.xlabel(\"Num of Epochs\")  \n",
        "plt.ylabel(\"Accuracy\")  \n",
        "plt.title(\"Training Accuracy vs Validation Accuracy\")  \n",
        "plt.legend(['train','validation'])"
      ],
      "metadata": {
        "id": "BQLMibaOpyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1)  \n",
        "plt.plot(cnn.history['loss'],'r')  \n",
        "plt.plot(cnn.history['val_loss'],'g')  \n",
        "plt.xticks(np.arange(0, 2, 100))  \n",
        "plt.rcParams['figure.figsize'] = (8, 6)  \n",
        "plt.xlabel(\"Num of Epochs\")  \n",
        "plt.ylabel(\"Loss\")  \n",
        "plt.title(\"Training Loss vs Validation Loss\")  \n",
        "plt.legend(['train','validation'])\n",
        "\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "Fh9vNoBTpza5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red 2: Se agrega Dropout\n",
        "\n",
        "Dropout es una técnica muy utilizada en redes neuronales para evitar el sobreajuste (overfitting). El sobreajuste es un problema común en el aprendizaje automático, donde el modelo se ajusta demasiado a los datos de entrenamiento y no es capaz de generalizar bien para datos nuevos y no vistos.\n",
        "\n",
        "Dropout consiste en apagar aleatoriamente (con cierta probabilidad) algunas de las neuronas en una capa durante el entrenamiento. Esto fuerza a la red a aprender características útiles en varias combinaciones de neuronas, lo que la hace más robusta y menos propensa al sobreajuste. Durante la inferencia, todas las neuronas están activas y se utilizan todas las combinaciones aprendidas durante el entrenamiento para hacer una predicción.\n",
        "\n",
        "En resumen, Dropout ayuda a prevenir el sobreajuste de la red al apagar aleatoriamente algunas de las neuronas en una capa durante el entrenamiento, lo que fuerza a la red a aprender características útiles en varias combinaciones de neuronas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gn_r2fv6tNc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar la CNN\n",
        "classifier_2 = Sequential()"
      ],
      "metadata": {
        "id": "XBhteTsOtih9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ol>\n",
        "<li>Conv2D: una capa de convolución 2D que aplica una convolución a la entrada. La convolución es una operación matemática que aplica un filtro a la entrada para producir una salida. En este caso, el filtro es un kernel de 3x3 y se utilizan 32 filtros. </li>\n",
        "\n",
        "<li>Conv2D aprende las características importantes de la imagen a través de la convolución.</li>\n",
        "\n",
        "<li>MaxPooling2D: una capa de agrupación que reduce la dimensión espacial de la salida de la capa anterior. En este caso, se utiliza un agrupamiento máximo de 2x2.</li>\n",
        "\n",
        "<li>Dropout: una técnica de regularización que ayuda a reducir el sobreajuste en la red neuronal. Durante el entrenamiento, esta capa apaga aleatoriamente algunas de las neuronas para que la red no se vuelva demasiado dependiente de ninguna neurona en particular. En este caso, el parámetro de dropout es 0.25, lo que significa que se apagan el 25% de las neuronas.</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "EpS0HsRDtr7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolución\n",
        "capa2_11=Conv2D(filters = 32,\n",
        "              padding='same',\n",
        "              kernel_size = (3, 3),\n",
        "              use_bias=1,\n",
        "              input_shape = (pixel, pixel, 1), \n",
        "              activation = \"relu\")\n",
        "capa2_12=MaxPooling2D(pool_size = (2,2))\n",
        "capa2_13=Dropout(0.25)\n"
      ],
      "metadata": {
        "id": "N8z9u0y_tsQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capa 2\n",
        "\n",
        "La capa Conv2D realiza la convolución de los datos de entrada con un conjunto de filtros que extraen características de las imágenes. En este caso, se definen 64 filtros con un tamaño de kernel de 3x3 y se especifica la función de activación ReLU.\n",
        "\n",
        "La capa MaxPooling2D reduce la dimensión espacial de la salida de la capa anterior, lo que ayuda a reducir la cantidad de parámetros y el tiempo de procesamiento. En este caso, se utiliza un tamaño de ventana de 2x2 para reducir la imagen a la mitad en ambas dimensiones.\n",
        "\n",
        "La capa Dropout aplica una técnica de regularización para evitar el sobreajuste del modelo. Durante el entrenamiento, se apaga aleatoriamente una fracción de las neuronas de la capa anterior para que no se dependa demasiado de una sola neurona. En este caso, se apaga el 25% de las neuronas en la capa anterior."
      ],
      "metadata": {
        "id": "-qCT6c2Ku6os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capa2_21=Conv2D(filters = 64,\n",
        "             padding='same',\n",
        "             kernel_size = (3, 3),\n",
        "             use_bias=1,\n",
        "             activation = \"relu\")\n",
        "\n",
        "capa2_22=MaxPooling2D(pool_size = (2,2))\n",
        "capa2_23=Dropout(0.25)"
      ],
      "metadata": {
        "id": "j-_Zyg_2u9Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "La capa Conv2D con filters = 128 realiza la convolución con 128 filtros de tamaño (3, 3) y con un padding de 'same', lo que significa que el tamaño de la salida será el mismo que el de la entrada, es decir (pixel, pixel, 128). La capa utiliza la función de activación relu que se encarga de eliminar los valores negativos de la salida de la capa anterior.\n",
        "\n",
        "La capa MaxPooling2D con pool_size = (2, 2) reduce a la mitad el tamaño de la salida de la capa anterior en ambas dimensiones (width y height) y así se disminuye la complejidad del modelo y se reduce el overfitting.\n",
        "\n",
        "La capa Dropout con una probabilidad de 0.25 descarta aleatoriamente el 25% de las salidas de la capa anterior, lo que ayuda a evitar el overfitting."
      ],
      "metadata": {
        "id": "DHJHEoisvMQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capa2_31=Conv2D(filters = 128,\n",
        "             padding='same',\n",
        "             kernel_size = (3, 3),\n",
        "             use_bias=1,\n",
        "             activation = \"relu\")\n",
        "capa2_32=MaxPooling2D(pool_size = (2,2))\n",
        "capa2_33=Dropout(0.25)"
      ],
      "metadata": {
        "id": "gQDROA0_vF0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas son tres capas que forman una estructura convolucional de la red neuronal. La capa Conv2D realiza una convolución 2D en la imagen de entrada. La convolución es una operación matemática que se utiliza para extraer características de la imagen de entrada. La capa MaxPooling2D realiza un submuestreo en la salida de la capa de convolución, reduciendo el tamaño de la salida y manteniendo sólo la información más importante. Por último, la capa Dropout se utiliza para prevenir el sobreajuste de la red. Apaga un porcentaje de las neuronas de la capa anterior durante el entrenamiento, lo que obliga a la red a aprender características más robustas y a evitar la memorización de los datos de entrenamiento."
      ],
      "metadata": {
        "id": "yz8gk8NJvYHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capa2_41=Conv2D(filters = 128,\n",
        "             padding='same',\n",
        "             kernel_size = (3, 3),\n",
        "             use_bias=1,\n",
        "             activation = \"relu\")\n",
        "capa2_42=MaxPooling2D(pool_size = (2,2))\n",
        "capa2_43=Dropout(0.25)"
      ],
      "metadata": {
        "id": "knHSuVmRvPWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas son las capas de la red neuronal densa que se utilizan después de las capas de convolución.\n",
        "\n",
        "La capa \"Flatten\" se utiliza para aplanar la matriz resultante de la última capa de convolución en un vector unidimensional, que luego se usa como entrada para las capas densas.\n",
        "\n",
        "La capa \"Dense\" es una capa completamente conectada en la que cada neurona está conectada a todas las neuronas de la capa anterior. En la primera capa densa (capa61) se utilizan 512 neuronas con función de activación ReLU. En la última capa densa (capa62) se utiliza una neurona con función de activación sigmoide para la clasificación binaria de perros o gatos."
      ],
      "metadata": {
        "id": "M200jCTwvhLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capa2_51=Flatten()\n",
        "capa2_61=Dense(512, use_bias=1, activation = \"relu\")\n",
        "capa2_62=Dense(1, use_bias=1, activation = \"sigmoid\")"
      ],
      "metadata": {
        "id": "A0o8oi1CvaMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_2.add(capa2_11)\n",
        "classifier_2.add(capa2_12)\n",
        "classifier_2.add(capa2_13)\n",
        "classifier_2.add(capa2_21)\n",
        "classifier_2.add(capa2_22)\n",
        "classifier_2.add(capa2_23)\n",
        "classifier_2.add(capa2_31)\n",
        "classifier_2.add(capa2_32)\n",
        "classifier_2.add(capa2_33)\n",
        "classifier_2.add(capa2_41)\n",
        "classifier_2.add(capa2_42)\n",
        "classifier_2.add(capa2_43)\n",
        "classifier_2.add(capa2_51)\n",
        "classifier_2.add(capa2_61)\n",
        "classifier_2.add(capa2_62)\n",
        "\n",
        "\n",
        "classifier_2.compile(loss = \"binary_crossentropy\",\n",
        "                   optimizer = \"adam\",\n",
        "                   #optimizer = \"sgd\",\n",
        "                   #optimizer = \"rmsprop\",\n",
        "                   metrics=['acc', 'mse'])\n",
        "\n",
        "classifier_2.summary()"
      ],
      "metadata": {
        "id": "3WjX97DcvkZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pasos_entrenamiento=train_generator.n//train_generator.batch_size\n",
        "#pasos_entrenamiento=1\n",
        "pasos_validacion=validation_generator.n//validation_generator.batch_size\n",
        "#pasos_validacion=1"
      ],
      "metadata": {
        "id": "ACYFRfFpvyI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\"\"\"\n",
        "cnn_2=classifier_2.fit(train_generator, #conjunto de entrenamiento\n",
        "               #y_test se toma de las etiquetas de los datos automaticamente\n",
        "               #https://stackoverflow.com/questions/62116637/y-argument-is-not-supported-when-using-keras-utils-sequence-as-input-error\n",
        "               #validation_generator, #conjunto de test\n",
        "               epochs=epocas, #Cuantas epocas usaremos para entrenar\n",
        "               steps_per_epoch=pasos_entrenamiento,\n",
        "               validation_data=(validation_generator),\n",
        "               validation_steps=pasos_validacion,\n",
        "               shuffle=True\n",
        "              ) \n",
        "#\"\"\""
      ],
      "metadata": {
        "id": "DrlAWq7tvzzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_2 = classifier_2.predict(validation_generator)\n",
        "y_pred_df_2 = pd.DataFrame(predictions_2)\n",
        "y_pred_df_2 =np.round(y_pred_df_2)\n",
        "\n",
        "\n",
        "true_classes_2 = validation_generator.classes\n",
        "y_test_df_2 = pd.DataFrame(true_classes_2)\n",
        "\n",
        "\n",
        "class_labels_2 = list(validation_generator.class_indices.keys())   \n",
        "\n",
        "\n",
        "#Train\n",
        "scores = classifier_2.evaluate(train_generator, steps=pasos_entrenamiento)\n",
        "print('TRAIN: ',\"\\n%s: %.2f%%\" % (classifier_2.metrics_names[1], scores[1]*100))\n",
        "\n",
        "#Validación\n",
        "scores = classifier_2.evaluate(validation_generator,steps=pasos_validacion)\n",
        "print('VALIDACIÓN: ',\"\\n%s: %.2f%%\" % (classifier_2.metrics_names[1], scores[1]*100))\n",
        "\n",
        "#Test\n",
        "scores = classifier_2.evaluate(test_generator,steps=pasos_validacion)\n",
        "print('TEST: ',\"\\n%s: %.2f%%\" % (classifier_2.metrics_names[1], scores[1]*100))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(y_pred_df_2, y_test_df_2, target_names=class_labels)\n",
        "print(report)   "
      ],
      "metadata": {
        "id": "lBCgWUMSJwab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red 3: BN-DA-Droout\n"
      ],
      "metadata": {
        "id": "n0MR_VFzv5pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,        #normalizado de la imagen\n",
        "                                   rotation_range=40,       #rotacion 40°   \n",
        "                                   width_shift_range=0.2,   #traslado vertical \n",
        "                                   height_shift_range=0.2,  #traslado horizontal\n",
        "                                   shear_range = 0.2,       #inclinación de la imagen \n",
        "                                   zoom_range = 0.2,        #aumento\n",
        "                                   horizontal_flip = True,  #flip horizontal\n",
        "                                   fill_mode = 'nearest'    #relleno de pixeles despues de transformacion\n",
        "                                  )   \n",
        "validation_datagen=ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "metadata": {
        "id": "Hh1rR8F5w_RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,    #directorio\n",
        "    target_size=(pixel, pixel),      #reescalado\n",
        "    color_mode=\"grayscale\",          #rgb color, grayscale grises   \n",
        "    batch_size=size,             #num de imagenes producidas por el generador de lotes \n",
        "    class_mode='binary',       #binary para 2 clases, categorical para mas de 2 clases\n",
        "    shuffle=True,              #mezclar el orden de la imagen que se está produciendo\n",
        "    seed=42                    #semilla aleatoria\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(pixel, pixel),      #reescalado\n",
        "    color_mode=\"grayscale\",          #rgb color, grayscale grises   \n",
        "    batch_size=size,             #num de imagenes producidas por el generador de lotes \n",
        "    class_mode='binary',       #binary para 2 clases, categorical para mas de 2 clases\n",
        "    shuffle=True,              #mezclar el orden de la imagen que se está produciendo\n",
        "    seed=42                    #semilla aleatoria\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(pixel, pixel),      #reescalado\n",
        "    color_mode=\"grayscale\",          #rgb color, grayscale grises   \n",
        "    batch_size=size,             #num de imagenes producidas por el generador de lotes \n",
        "    class_mode='binary',       #binary para 2 clases, categorical para mas de 2 clases\n",
        "    shuffle=True,              #mezclar el orden de la imagen que se está produciendo\n",
        "    seed=42                    #semilla aleatoria\n",
        ")"
      ],
      "metadata": {
        "id": "yV0AMkqaxJOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Esta red es similar a las anteriores, con algunas diferencias en las capas. Aquí se agregaron capas de regularización como Batch Normalization y Dropout para reducir el overfitting.\n",
        "\n",
        "La primera capa convolucional tiene 32 filtros, y se utiliza el padding \"same\" para que el tamaño de la imagen de salida sea el mismo que el de entrada. También se agrega una capa de MaxPooling con un tamaño de ventana de (2,2) para reducir el tamaño de la imagen a la mitad. Luego se agrega una capa de Dropout para regularizar el modelo.\n",
        "\n",
        "La segunda capa convolucional tiene 64 filtros, seguida por una capa de MaxPooling y otra de Dropout.\n",
        "\n",
        "La tercera capa convolucional tiene 128 filtros, seguida por una capa de MaxPooling y otra de Dropout.\n",
        "\n",
        "La cuarta capa convolucional es similar a la tercera, con 128 filtros, y se agrega una capa de MaxPooling y Dropout.\n",
        "\n",
        "Luego se agrega una capa de Flattening para convertir la imagen en un vector, y dos capas densas para la clasificación.\n",
        "\n",
        "Finalmente, el modelo se compila con la función de pérdida de entropía cruzada binaria, el optimizador Adam y se mide la precisión y el error cuadrático medio."
      ],
      "metadata": {
        "id": "qqFxL9rRy9xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_3 = Sequential()\n",
        "# Convolución\n",
        "capa3_11=Conv2D(filters = 32,\n",
        "              padding='same',\n",
        "              kernel_size = (3, 3),\n",
        "              use_bias=1,\n",
        "              input_shape = (pixel, pixel, 1), \n",
        "              activation = \"relu\")\n",
        "capa3_12=MaxPooling2D(pool_size = (2,2))\n",
        "capa3_13=Dropout(0.25)\n",
        "\n",
        "#capa 2\n",
        "capa3_21=Conv2D(filters = 64,\n",
        "             padding='same',\n",
        "             kernel_size = (3, 3),\n",
        "             use_bias=1,\n",
        "             activation = \"relu\")\n",
        "capa3_22=MaxPooling2D(pool_size = (2,2))\n",
        "capa3_23=Dropout(0.25)\n",
        "\n",
        "#Capa 3\n",
        "\n",
        "capa3_31=Conv2D(filters = 128,\n",
        "             padding='same',\n",
        "             kernel_size = (3, 3),\n",
        "             use_bias=1,\n",
        "             activation = \"relu\")\n",
        "capa3_32=MaxPooling2D(pool_size = (2,2))\n",
        "capa3_33=Dropout(0.25)\n",
        "\n",
        "#Capa 4\n",
        "capa3_41=Conv2D(filters = 128,\n",
        "             padding='same',\n",
        "             kernel_size = (3, 3),\n",
        "             use_bias=1,\n",
        "             activation = \"relu\")\n",
        "capa3_42=MaxPooling2D(pool_size = (2,2))\n",
        "capa3_43=Dropout(0.25)\n",
        "\n",
        "#Flattening\n",
        "capa3_51=Flatten()\n",
        "#Full conected\n",
        "capa3_61=Dense(512, use_bias=1, activation = \"relu\")\n",
        "capa3_62=Dense(1, use_bias=1, activation = \"sigmoid\")\n",
        "\n",
        "classifier_3.add(capa3_11)\n",
        "classifier_3.add(capa3_12)\n",
        "classifier_3.add(capa3_21)\n",
        "classifier_3.add(capa3_22)\n",
        "classifier_3.add(capa3_31)\n",
        "classifier_3.add(capa3_32)\n",
        "classifier_3.add(capa3_41)\n",
        "classifier_3.add(capa3_42)\n",
        "classifier_3.add(capa3_51)\n",
        "classifier_3.add(capa3_61)\n",
        "classifier_3.add(capa3_62)\n",
        "\n",
        "\n",
        "classifier_3.compile(loss = \"binary_crossentropy\",\n",
        "                   optimizer = \"adam\",\n",
        "                   #optimizer = \"sgd\",\n",
        "                   #optimizer = \"rmsprop\",\n",
        "                   metrics=['acc', 'mse'])\n",
        "\n",
        "classifier_3.summary()"
      ],
      "metadata": {
        "id": "xzDa--sHxePU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pasos_entrenamiento=train_generator.n//train_generator.batch_size\n",
        "#pasos_entrenamiento=1\n",
        "pasos_validacion=validation_generator.n//validation_generator.batch_size\n",
        "#pasos_validacion=1\n",
        "\n",
        "#\"\"\"\n",
        "cnn_3=classifier_3.fit(train_generator, #conjunto de entrenamiento\n",
        "               #y_test se toma de las etiquetas de los datos automaticamente\n",
        "               #https://stackoverflow.com/questions/62116637/y-argument-is-not-supported-when-using-keras-utils-sequence-as-input-error\n",
        "               #validation_generator, #conjunto de test\n",
        "               epochs=epocas, #Cuantas epocas usaremos para entrenar\n",
        "               steps_per_epoch=pasos_entrenamiento,\n",
        "               validation_data=(validation_generator),\n",
        "               validation_steps=pasos_validacion,\n",
        "               shuffle=True\n",
        "              ) \n",
        "#\"\"\""
      ],
      "metadata": {
        "id": "DsutqcEexz5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_3 = classifier_3.predict(validation_generator)\n",
        "y_pred_df_3 = pd.DataFrame(predictions_3)\n",
        "y_pred_df_3 =np.round(y_pred_df_3)\n",
        "\n",
        "\n",
        "true_classes_3 = validation_generator.classes\n",
        "y_test_df_3 = pd.DataFrame(true_classes_3)\n",
        "\n",
        "\n",
        "class_labels_3 = list(validation_generator.class_indices.keys())   \n",
        "\n",
        "\n",
        "#Train\n",
        "scores = classifier_3.evaluate(train_generator, steps=pasos_entrenamiento)\n",
        "print('TRAIN: ',\"\\n%s: %.2f%%\" % (classifier_3.metrics_names[1], scores[1]*100))\n",
        "\n",
        "#Validación\n",
        "scores = classifier_3.evaluate(validation_generator,steps=pasos_validacion)\n",
        "print('VALIDACIÓN: ',\"\\n%s: %.2f%%\" % (classifier_3.metrics_names[1], scores[1]*100))\n",
        "\n",
        "#Test\n",
        "scores = classifier_3.evaluate(test_generator,steps=pasos_validacion)\n",
        "print('TEST: ',\"\\n%s: %.2f%%\" % (classifier_3.metrics_names[1], scores[1]*100))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(y_pred_df_3, y_test_df_3, target_names=class_labels)\n",
        "print(report)   "
      ],
      "metadata": {
        "id": "6cLuzy7CMxLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Red 4: RGB-VGG16\n",
        "\n",
        "La arquitectura de la red neuronal convolucional VGG16 es un modelo de aprendizaje profundo muy utilizado para la clasificación de imágenes. Esta red utiliza capas de convolución de tamaño reducido, seguidas de capas de pooling, y varias capas totalmente conectadas al final.\n",
        "\n",
        "La arquitectura de la red VGG16 consta de varias capas convolucionales y capas totalmente conectadas, con un total de 16 capas, de ahí su nombre. La primera capa convolucional utiliza un kernel de tamaño 3x3, mientras que las capas subsiguientes utilizan un kernel de tamaño 2x2.\n",
        "\n",
        "La red VGG16 es conocida por su alta precisión en la clasificación de imágenes, y se utiliza a menudo como una red pre-entrenada para tareas de clasificación de imágenes en conjuntos de datos grandes, como ImageNet.\n",
        "\n",
        "Para implementar una red VGG16 en Keras, se puede utilizar la clase \"VGG16\" proporcionada por la biblioteca Keras. Esta clase se importa desde el módulo \"keras.applications\", y puede ser instanciada con o sin pesos pre-entrenados.\n",
        "\n",
        "En este caso, al tratarse de una red RGB-VGG16, se utilizará la misma arquitectura, pero con imágenes a color (RGB) en lugar de imágenes en escala de grises."
      ],
      "metadata": {
        "id": "RyHHwIDpzbTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "validation_datagen=ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,    #directorio\n",
        "    target_size=(pixel, pixel),      #reescalado\n",
        "    color_mode=\"rgb\",          #rgb color, grayscale grises   \n",
        "    batch_size=size,             #num de imagenes producidas por el generador de lotes \n",
        "    class_mode='binary',       #binary para 2 clases, categorical para mas de 2 clases\n",
        "    shuffle=True,              #mezclar el orden de la imagen que se está produciendo\n",
        "    seed=42                    #semilla aleatoria\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(pixel, pixel),      #reescalado\n",
        "    color_mode=\"rgb\",          #rgb color, grayscale grises   \n",
        "    batch_size=size,             #num de imagenes producidas por el generador de lotes \n",
        "    class_mode='binary',       #binary para 2 clases, categorical para mas de 2 clases\n",
        "    shuffle=True,              #mezclar el orden de la imagen que se está produciendo\n",
        "    seed=42                    #semilla aleatoria\n",
        ")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(pixel, pixel),      #reescalado\n",
        "    color_mode=\"rgb\",          #rgb color, grayscale grises   \n",
        "    batch_size=size,             #num de imagenes producidas por el generador de lotes \n",
        "    class_mode='binary',       #binary para 2 clases, categorical para mas de 2 clases\n",
        "    shuffle=True,              #mezclar el orden de la imagen que se está produciendo\n",
        "    seed=42                    #semilla aleatoria\n",
        ")"
      ],
      "metadata": {
        "id": "gizkb4WIzmSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La red 4 es una implementación de la red VGG16 utilizando transferencia de aprendizaje. Se carga la estructura y los pesos pre-entrenados de la red VGG16, se congelan todas las capas y se añaden capas adicionales para adaptar la red al problema de clasificación binaria de perros y gatos.\n",
        "\n",
        "En este caso, se utiliza la función VGG16 de la librería tensorflow.keras.applications para cargar la estructura y los pesos pre-entrenados de la red VGG16. Luego, se itera sobre todas las capas de la red y se establece trainable=False para congelar todas las capas y evitar que se modifiquen sus pesos durante el entrenamiento.\n",
        "\n",
        "Finalmente, se añaden dos capas completamente conectadas (Dense) para adaptar la red a la tarea de clasificación binaria y se compila el modelo utilizando la función de pérdida de entropía cruzada binaria (binary_crossentropy), el optimizador Adam y se monitorean la exactitud (acc) y el error cuadrático medio (mse) como métricas de desempeño."
      ],
      "metadata": {
        "id": "r3kDLRnd0tpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "classifier_4 = Sequential()\n",
        "\n",
        "Capa4_Pre=VGG16(input_shape = (pixel, pixel, 3),\n",
        "                 include_top=False,\n",
        "                weights='imagenet')\n",
        "\n",
        "for layer in Capa4_Pre.layers:\n",
        "    layer.trainable=False\n",
        "\n",
        "capa4_51=Flatten()\n",
        "capa4_61=Dense(512, use_bias=1, activation = \"relu\")\n",
        "capa4_62=Dense(1, use_bias=1, activation = \"sigmoid\")\n",
        "\n",
        "classifier_4.add(Capa4_Pre)\n",
        "classifier_4.add(capa4_51)\n",
        "classifier_4.add(capa4_61)\n",
        "classifier_4.add(capa4_62)\n",
        "\n",
        "classifier_4.compile(loss = \"binary_crossentropy\",\n",
        "                   optimizer = \"adam\",\n",
        "                   #optimizer = \"sgd\",\n",
        "                   #optimizer = \"rmsprop\",\n",
        "                   metrics=['acc', 'mse'])\n",
        "classifier_4.summary()"
      ],
      "metadata": {
        "id": "pBFqOdn10ARk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pasos_entrenamiento=train_generator.n//train_generator.batch_size\n",
        "#pasos_entrenamiento=1\n",
        "pasos_validacion=validation_generator.n//validation_generator.batch_size\n",
        "#pasos_validacion=1\n",
        "#\"\"\"\n",
        "cnn_4=classifier_4.fit(train_generator, #conjunto de entrenamiento\n",
        "               #y_test se toma de las etiquetas de los datos automaticamente\n",
        "               #https://stackoverflow.com/questions/62116637/y-argument-is-not-supported-when-using-keras-utils-sequence-as-input-error\n",
        "               #validation_generator, #conjunto de test\n",
        "               epochs=epocas, #Cuantas epocas usaremos para entrenar\n",
        "               steps_per_epoch=pasos_entrenamiento,\n",
        "               validation_data=(validation_generator),\n",
        "               validation_steps=pasos_validacion,\n",
        "               shuffle=True\n",
        "              ) \n",
        "#\"\"\""
      ],
      "metadata": {
        "id": "yVIRLN6E0v5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_4 = classifier_4.predict(validation_generator)\n",
        "y_pred_df_4 = pd.DataFrame(predictions_4)\n",
        "y_pred_df_4 =np.round(y_pred_df_4)\n",
        "\n",
        "\n",
        "true_classes_4 = validation_generator.classes\n",
        "y_test_df_4 = pd.DataFrame(true_classes_4)\n",
        "\n",
        "\n",
        "class_labels_4 = list(validation_generator.class_indices.keys())   \n",
        "\n",
        "\n",
        "#Train\n",
        "scores = classifier_4.evaluate(train_generator, steps=pasos_entrenamiento)\n",
        "print('TRAIN: ',\"\\n%s: %.2f%%\" % (classifier_4.metrics_names[1], scores[1]*100))\n",
        "\n",
        "#Validación\n",
        "scores = classifier_4.evaluate(validation_generator,steps=pasos_validacion)\n",
        "print('VALIDACIÓN: ',\"\\n%s: %.2f%%\" % (classifier_4.metrics_names[1], scores[1]*100))\n",
        "\n",
        "#Test\n",
        "scores = classifier_4.evaluate(test_generator,steps=pasos_validacion)\n",
        "print('TEST: ',\"\\n%s: %.2f%%\" % (classifier_4.metrics_names[1], scores[1]*100))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(y_pred_df_4, y_test_df_4, target_names=class_labels)\n",
        "print(report)   "
      ],
      "metadata": {
        "id": "gJpSIOvaNC0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validar predicciones"
      ],
      "metadata": {
        "id": "ZD4VNkNy1yTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histories = {\"CNN\": cnn.history, \"CNN2\": cnn_2.history, \"CNN3\": cnn_3.history, \"CNN4\": cnn_4.history}\n",
        "\n",
        "plt.figure(0)\n",
        "for name, history in histories.items():\n",
        "    plt.plot(history['acc'], label=name)\n",
        "    plt.plot(history['val_acc'], label=name + ' val')\n",
        "plt.xticks(np.arange(0, 20, 100))\n",
        "plt.rcParams['figure.figsize'] = (8, 6)\n",
        "plt.xlabel(\"Num of Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "vfQpGEb82D4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una figura con dos subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Graficar la precisión de entrenamiento y validación para cada modelo\n",
        "axs[0, 0].plot(cnn.history['acc'], 'r')\n",
        "axs[0, 0].plot(cnn.history['val_acc'], 'g')\n",
        "axs[0, 0].set_xlabel(\"Num of Epochs\")\n",
        "axs[0, 0].set_ylabel(\"Accuracy\")\n",
        "axs[0, 0].set_title(\"Modelo 1\")\n",
        "axs[0, 0].legend(['train', 'validation'])\n",
        "\n",
        "axs[0, 1].plot(cnn_2.history['acc'], 'r')\n",
        "axs[0, 1].plot(cnn_2.history['val_acc'], 'g')\n",
        "axs[0, 1].set_xlabel(\"Num of Epochs\")\n",
        "axs[0, 1].set_ylabel(\"Accuracy\")\n",
        "axs[0, 1].set_title(\"Modelo 2\")\n",
        "axs[0, 1].legend(['train', 'validation'])\n",
        "\n",
        "axs[1, 0].plot(cnn_3.history['acc'], 'r')\n",
        "axs[1, 0].plot(cnn_3.history['val_acc'], 'g')\n",
        "axs[1, 0].set_xlabel(\"Num of Epochs\")\n",
        "axs[1, 0].set_ylabel(\"Accuracy\")\n",
        "axs[1, 0].set_title(\"Modelo 3\")\n",
        "axs[1, 0].legend(['train', 'validation'])\n",
        "\n",
        "axs[1, 1].plot(cnn_4.history['acc'], 'r')\n",
        "axs[1, 1].plot(cnn_4.history['val_acc'], 'g')\n",
        "axs[1, 1].set_xlabel(\"Num of Epochs\")\n",
        "axs[1, 1].set_ylabel(\"Accuracy\")\n",
        "axs[1, 1].set_title(\"Modelo 4\")\n",
        "axs[1, 1].legend(['train', 'validation'])\n",
        "\n",
        "# Ajustar la separación entre subplots\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2LbSm8H32KO6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}