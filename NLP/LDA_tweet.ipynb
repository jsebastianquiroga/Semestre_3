{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m4cmOojch_to",
        "gKiIShYiXv1C",
        "ydlq9_vRaUdQ",
        "yfxtbQ_2ZrWL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsebastianquiroga/analitica_ia_puj/blob/main/NLP/LDA_tweet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import spacy "
      ],
      "metadata": {
        "id": "pwxfTDdRm3dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://hipertextual.com/2019/03/capitana-marvel\"\n",
        "\n",
        "# Hacer la petición GET a la página de Wikipedia\n",
        "respuesta = requests.get(url)\n",
        "\n",
        "# Extraer el contenido HTML de la página\n",
        "contenido_html = respuesta.content\n",
        "\n",
        "# Convertir el contenido HTML en un objeto BeautifulSoup\n",
        "soup = BeautifulSoup(contenido_html, 'html.parser')\n",
        "\n",
        "# Encontrar y extraer el contenido de la etiqueta 'div' con el atributo 'class' igual a 'entry-content'\n",
        "contenido = soup.find('div', attrs={'class': 'entry-content'})\n",
        "\n",
        "# Extraer el texto del contenido\n",
        "if contenido:\n",
        "    texto = contenido.get_text()\n",
        "    # print(texto)\n",
        "else:\n",
        "    print(\"Content not found.\")\n"
      ],
      "metadata": {
        "id": "2bfME_pmm52d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resenaCMarvel = texto\n",
        "resenaCMarvel_s = texto"
      ],
      "metadata": {
        "id": "SRpE4JXKoXvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "fichasCMarvel=nltk.tokenize.word_tokenize(resenaCMarvel_s,language=\"spanish\")"
      ],
      "metadata": {
        "id": "hBH2rbFupVGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mjcg2_Qup6Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "Yf0X-B9To1CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "# Tokenize the text using spaCy\n",
        "doc = nlp(resenaCMarvel)\n",
        "\n",
        "# Extract the tokens (corpora) from the doc object\n",
        "tokens = [token.text for token in doc]"
      ],
      "metadata": {
        "id": "syi4yocDxlWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agregar otro texto, como la divina comedia, se imprimiera con un _2 que las anteriores variables."
      ],
      "metadata": {
        "id": "5lUo7AUMqfUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://es.wikipedia.org/wiki/Divina_comedia\"\n",
        "\n",
        "# Hacer la petición GET a la página de Wikipedia\n",
        "respuesta = requests.get(url)\n",
        "\n",
        "# Extraer el contenido HTML de la página\n",
        "contenido_html = respuesta.content\n",
        "\n",
        "# Convertir el contenido HTML en un objeto BeautifulSoup\n",
        "soup = BeautifulSoup(contenido_html, 'html.parser')\n",
        "\n",
        "# Encontrar y extraer el contenido de la etiqueta 'div' con el atributo 'class' igual a 'mw-parser-output'\n",
        "contenido = soup.find('div', attrs={'class': 'mw-parser-output'})\n",
        "\n",
        "# Extraer el texto del contenido\n",
        "if contenido:\n",
        "    texto_2 = contenido.get_text()\n",
        "    # print(texto)\n",
        "else:\n",
        "    print(\"Content not found.\")\n",
        "\n",
        "divinacomedia = texto_2\n",
        "divinacomedia_s = texto_2"
      ],
      "metadata": {
        "id": "2q4cFTCYtVxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se tokeniza\n",
        "tokens_divina=nltk.tokenize.word_tokenize(divinacomedia,language=\"spanish\")\n",
        "# fichasCMarvel\n",
        "\n",
        "corporaDivina=nltk.Text(tokens_divina)\n",
        "corporaDivina\n",
        "\n",
        "# Tokenize the divinacomedia_s using spaCy\n",
        "doc_2 = nlp(divinacomedia_s)\n",
        "\n",
        "# Extract the tokens (corpora) from the doc object\n",
        "tokens = [token.text for token in doc_2]"
      ],
      "metadata": {
        "id": "U5KoCB6_uUKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "1H3gPioP2joc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis por frase"
      ],
      "metadata": {
        "id": "qtGcO0f30_6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentfrase=[]\n",
        "frases=nltk.tokenize.sent_tokenize(resenaCMarvel)"
      ],
      "metadata": {
        "id": "GgSWuJM_xbTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for oracion in frases:\n",
        "    palabras=nltk.tokenize.word_tokenize(oracion,language=\"spanish\")\n",
        "    valsent= sum(sentimientos.get(palabra,0) for palabra in palabras)\n",
        "    sentfrase.append(valsent)"
      ],
      "metadata": {
        "id": "vYWnUToI1SbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_divina"
      ],
      "metadata": {
        "id": "Du7Mdt091TXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-cp9US01sJB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}