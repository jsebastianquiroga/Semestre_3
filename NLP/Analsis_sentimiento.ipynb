{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m4cmOojch_to",
        "gKiIShYiXv1C",
        "ydlq9_vRaUdQ",
        "yfxtbQ_2ZrWL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsebastianquiroga/Semestre_3/blob/main/NLP/Analsis_sentimiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import spacy "
      ],
      "metadata": {
        "id": "pwxfTDdRm3dm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://hipertextual.com/2019/03/capitana-marvel\"\n",
        "\n",
        "# Hacer la petición GET a la página de Wikipedia\n",
        "respuesta = requests.get(url)\n",
        "\n",
        "# Extraer el contenido HTML de la página\n",
        "contenido_html = respuesta.content\n",
        "\n",
        "# Convertir el contenido HTML en un objeto BeautifulSoup\n",
        "soup = BeautifulSoup(contenido_html, 'html.parser')\n",
        "\n",
        "# Encontrar y extraer el contenido de la etiqueta 'div' con el atributo 'class' igual a 'entry-content'\n",
        "contenido = soup.find('div', attrs={'class': 'entry-content'})\n",
        "\n",
        "# Extraer el texto del contenido\n",
        "if contenido:\n",
        "    texto = contenido.get_text()\n",
        "    # print(texto)\n",
        "else:\n",
        "    print(\"Content not found.\")\n"
      ],
      "metadata": {
        "id": "2bfME_pmm52d"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resenaCMarvel = texto\n",
        "resenaCMarvel_s = texto"
      ],
      "metadata": {
        "id": "SRpE4JXKoXvt"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "hBH2rbFupVGD",
        "outputId": "ec86c2a0-9050-481c-f46d-668446f78ff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fichasCMarvel=nltk.tokenize.word_tokenize(resenaCMarvel_s,language=\"spanish\")"
      ],
      "metadata": {
        "id": "CcdAkcKxogQH"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mjcg2_Qup6Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "Yf0X-B9To1CM",
        "outputId": "cfc0b35c-4a35-45b5-9141-9e621616e3b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-23 06:18:50.570062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.5.0/es_core_news_sm-3.5.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from es-core-news-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->es-core-news-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "# Tokenize the text using spaCy\n",
        "doc = nlp(resenaCMarvel)\n",
        "\n",
        "# Extract the tokens (corpora) from the doc object\n",
        "tokens = [token.text for token in doc]"
      ],
      "metadata": {
        "id": "syi4yocDxlWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agregar otro texto, como la divina comedia, se imprimiera con un _2 que las anteriores variables."
      ],
      "metadata": {
        "id": "5lUo7AUMqfUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://es.wikipedia.org/wiki/Divina_comedia\"\n",
        "\n",
        "# Hacer la petición GET a la página de Wikipedia\n",
        "respuesta = requests.get(url)\n",
        "\n",
        "# Extraer el contenido HTML de la página\n",
        "contenido_html = respuesta.content\n",
        "\n",
        "# Convertir el contenido HTML en un objeto BeautifulSoup\n",
        "soup = BeautifulSoup(contenido_html, 'html.parser')\n",
        "\n",
        "# Encontrar y extraer el contenido de la etiqueta 'div' con el atributo 'class' igual a 'mw-parser-output'\n",
        "contenido = soup.find('div', attrs={'class': 'mw-parser-output'})\n",
        "\n",
        "# Extraer el texto del contenido\n",
        "if contenido:\n",
        "    texto_2 = contenido.get_text()\n",
        "    # print(texto)\n",
        "else:\n",
        "    print(\"Content not found.\")\n"
      ],
      "metadata": {
        "id": "2q4cFTCYtVxv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "divinacomedia = texto_2\n",
        "#Se tokeniza\n",
        "tokens_divina=nltk.tokenize.word_tokenize(divinacomedia,language=\"spanish\")\n",
        "# fichasCMarvel\n",
        "\n",
        "corporaDivina=nltk.Text(tokens_divina)\n",
        "corporaDivina\n",
        "\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# Create a new Doc object from the list of tokens and the nlp.vocab\n",
        "corporaDivina_s = Doc(nlp.vocab, words=fichasCMarvel_s)\n",
        "\n",
        "# print(doc_2)"
      ],
      "metadata": {
        "id": "U5KoCB6_uUKj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corporaDivina_s"
      ],
      "metadata": {
        "id": "q_VDkRz6xDEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GgSWuJM_xbTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}