{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsebastianquiroga/analitica_ia_puj/blob/main/Tesis/notebook_listado_estudiantes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3FEiFZVK2aZZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "import statistics\n",
        "import warnings\n",
        "import re\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import display\n",
        "warnings.filterwarnings('ignore')\n",
        "# from pandas_profiling import ProfileReport"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Exploración de datos."
      ],
      "metadata": {
        "id": "ZmNd38cZmjNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6lEOYli62aZa",
        "outputId": "03ebc765-e7e4-4981-9aa7-d32386020ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2ccb871f0aff>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cargar tus datos en un DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Listado Estudiantes.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Listado Estudiantes.csv'"
          ]
        }
      ],
      "source": [
        "# Cargar tus datos en un DataFrame\n",
        "df = pd.read_csv(\"/content/Listado Estudiantes.csv\", delimiter=';', encoding='latin_1')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 Descripción de datos"
      ],
      "metadata": {
        "id": "IW5P19a0msGY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z65VIQNf2aZa"
      },
      "outputs": [],
      "source": [
        "def describe_data(df):\n",
        "    # Inicializar DataFrame vacío\n",
        "    description_df = pd.DataFrame()\n",
        "\n",
        "    # Agregar columnas una por una al DataFrame\n",
        "    description_df['Nombre de cada campo o columna'] = df.columns\n",
        "    description_df['Descripción'] = [''] * len(df.columns)  # Inicializar con cadenas vacías\n",
        "    description_df['Tipo de dato'] = [str(dtype) for dtype in df.dtypes]\n",
        "    description_df['Cantidad de registros no vacíos'] = df.notna().sum().values\n",
        "    description_df['Cantidad de registros vacíos'] = df.isna().sum().values\n",
        "\n",
        "    return description_df\n",
        "\n",
        "# Llamar a la función\n",
        "data_description_df = describe_data(df)\n",
        "\n",
        "# Luego, puedes agregar las descripciones manualmente como en el paso 5 de la explicación anterior.\n",
        "\n",
        "data_description_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_data(df):\n",
        "    # Inicializar DataFrame vacío\n",
        "    description_df = pd.DataFrame()\n",
        "\n",
        "    # Agregar columnas una por una al DataFrame\n",
        "    description_df['Nombre variable'] = df.columns\n",
        "    description_df['Descripción'] = [''] * len(df.columns)  # Inicializar con cadenas vacías\n",
        "    description_df['Tipo'] = [str(dtype) for dtype in df.dtypes]\n",
        "    description_df['# de registros'] = df.notna().sum().values\n",
        "    description_df['# de registros vacíos'] = df.isna().sum().values\n",
        "    description_df['# de valores únicos'] = df.nunique().values\n",
        "\n",
        "    # Inicializar columnas de rango y moda/mediana con valores vacíos\n",
        "    description_df['Rango'] = [''] * len(df.columns)\n",
        "    description_df['Moda/Mediana'] = [''] * len(df.columns)\n",
        "\n",
        "    for idx, column in enumerate(df.columns):\n",
        "        # Si la columna es numérica, calcular el rango\n",
        "        if pd.api.types.is_numeric_dtype(df[column]):\n",
        "            description_df.loc[idx, 'Rango'] = f'{df[column].min()} - {df[column].max()}'\n",
        "            description_df.loc[idx, 'Moda/Mediana'] = df[column].median()\n",
        "        # Si la columna no es numérica, calcular la moda\n",
        "        else:\n",
        "            description_df.loc[idx, 'Moda/Mediana'] = df[column].mode().iloc[0] if not df[column].mode().empty else ''\n",
        "\n",
        "    return description_df\n",
        "\n",
        "# Llamar a la función\n",
        "data_description_df = describe_data(df)\n",
        "\n",
        "# Luego, puedes agregar las descripciones manualmente como en el paso 5 de la explicación anterior.\n",
        "data_description_df"
      ],
      "metadata": {
        "id": "qs7T9KakEeq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Exploracion de datos."
      ],
      "metadata": {
        "id": "p2Y4NVNpmcq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "XCVY78pGoKq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vesgBJbj2aZa"
      },
      "outputs": [],
      "source": [
        "df['semestre'] = df['Año Semestre'].str.split('-').str.get(1)\n",
        "df['ano'] = df['Año Semestre'].str.split('-').str.get(0)\n",
        "\n",
        "# Importa las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configura el estilo de Seaborn\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Define una paleta de colores más oscura\n",
        "colores_oscuros = sns.color_palette(\"dark\", as_cmap=True)\n",
        "\n",
        "# Suponiendo que df es tu DataFrame\n",
        "# Asegúrate de que 'ano' es una columna numérica\n",
        "df['ano'] = pd.to_numeric(df['ano'], errors='coerce')\n",
        "\n",
        "# Agrupa los datos por 'ano' y 'Rectoría/Vicerrectoría', y cuenta la cantidad de estudiantes\n",
        "datos_agrupados = df.groupby(['ano', 'Rectoría/Vicerrectoría'])['Código Estudiante Banner'].count().reset_index()\n",
        "\n",
        "# Crea la gráfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.lineplot(data=datos_agrupados, x='ano', y='Código Estudiante Banner', hue='Rectoría/Vicerrectoría', palette=colores_oscuros, lw=2)\n",
        "plt.title('Cantidad de estudiantes por año y Rectoría/Vicerrectoría', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Año', fontsize=16)\n",
        "plt.ylabel('Cantidad de Estudiantes', fontsize=16)\n",
        "plt.legend(title='Rectoría/Vicerrectoría', title_fontsize='14', fontsize='12', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
        "plt.xticks(rotation=45)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configura el estilo de Seaborn\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Define una paleta de colores más oscura\n",
        "colores_oscuros = sns.color_palette(\"dark\", as_cmap=True)\n",
        "\n",
        "# Suponiendo que df es tu DataFrame\n",
        "# Asegúrate de que 'ano' es una columna numérica\n",
        "df['ano'] = pd.to_numeric(df['ano'], errors='coerce')\n",
        "\n",
        "# Agrupa los datos por 'ano' y 'Nivel Formación', y cuenta la cantidad de estudiantes\n",
        "datos_agrupados = df.groupby(['ano', 'Nivel Formación'])['Código Estudiante Banner'].count().reset_index()\n",
        "\n",
        "# Crea la gráfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.lineplot(data=datos_agrupados, x='ano', y='Código Estudiante Banner', hue='Nivel Formación', palette=colores_oscuros, lw=2)\n",
        "plt.title('Cantidad de estudiantes por año y Nivel Formación', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Año', fontsize=16)\n",
        "plt.ylabel('Cantidad de Estudiantes', fontsize=16)\n",
        "plt.legend(title='Nivel Formación', title_fontsize='14', fontsize='12', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
        "plt.xticks(rotation=45)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HG6MaxDstX5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configura el estilo de Seaborn\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Define una paleta de colores más oscura\n",
        "colores_oscuros = sns.color_palette(\"dark\", as_cmap=True)\n",
        "\n",
        "# Suponiendo que df es tu DataFrame\n",
        "# Asegúrate de que 'ano' es una columna numérica\n",
        "df['ano'] = pd.to_numeric(df['ano'], errors='coerce')\n",
        "\n",
        "# Agrupa los datos por 'ano' y 'Jornada', y cuenta la cantidad de estudiantes\n",
        "datos_agrupados = df.groupby(['ano', 'Jornada'])['Código Estudiante Banner'].count().reset_index()\n",
        "\n",
        "# Crea la gráfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.lineplot(data=datos_agrupados, x='ano', y='Código Estudiante Banner', hue='Jornada', palette=colores_oscuros, lw=2)\n",
        "plt.title('Cantidad de estudiantes por año y Jornada', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Año', fontsize=16)\n",
        "plt.ylabel('Cantidad de Estudiantes', fontsize=16)\n",
        "plt.legend(title='Jornada', title_fontsize='14', fontsize='12', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
        "plt.xticks(rotation=45)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eSAT54I8w9iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configura el estilo de Seaborn\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Define una paleta de colores más oscura\n",
        "colores_oscuros = sns.color_palette(\"dark\", as_cmap=True)\n",
        "\n",
        "# Suponiendo que df es tu DataFrame\n",
        "# Asegúrate de que 'ano' es una columna numérica\n",
        "df['ano'] = pd.to_numeric(df['ano'], errors='coerce')\n",
        "\n",
        "# Agrupa los datos por 'ano' y 'Area Conocimiento', y cuenta la cantidad de estudiantes\n",
        "datos_agrupados = df.groupby(['ano', 'Area Conocimiento'])['Código Estudiante Banner'].count().reset_index()\n",
        "\n",
        "# Crea la gráfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.lineplot(data=datos_agrupados, x='ano', y='Código Estudiante Banner', hue='Area Conocimiento', palette=colores_oscuros, lw=2)\n",
        "plt.title('Cantidad de estudiantes por año y Area Conocimiento', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Año', fontsize=16)\n",
        "plt.ylabel('Cantidad de Estudiantes', fontsize=16)\n",
        "plt.legend(title='Area Conocimiento', title_fontsize='14', fontsize='12', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
        "plt.xticks(rotation=45)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ssLe3C2oygvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configura el estilo de Seaborn\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Define una paleta de colores más oscura\n",
        "colores_oscuros = sns.color_palette(\"dark\", as_cmap=True)\n",
        "\n",
        "# Suponiendo que df es tu DataFrame\n",
        "# Asegúrate de que 'ano' es una columna numérica\n",
        "df['ano'] = pd.to_numeric(df['ano'], errors='coerce')\n",
        "\n",
        "# Agrupa los datos por 'ano' y 'Género', y cuenta la cantidad de estudiantes\n",
        "datos_agrupados = df.groupby(['ano', 'Género'])['Código Estudiante Banner'].count().reset_index()\n",
        "\n",
        "# Crea la gráfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.lineplot(data=datos_agrupados, x='ano', y='Código Estudiante Banner', hue='Género', palette=colores_oscuros, lw=2)\n",
        "plt.title('Cantidad de estudiantes por año y Género', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Año', fontsize=16)\n",
        "plt.ylabel('Cantidad de Estudiantes', fontsize=16)\n",
        "plt.legend(title='Género', title_fontsize='14', fontsize='12', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
        "plt.xticks(rotation=45)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3Qso1cQN8JNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configura el estilo de Seaborn\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Define una paleta de colores más oscura\n",
        "colores_oscuros = sns.color_palette(\"dark\", as_cmap=True)\n",
        "\n",
        "# Suponiendo que df es tu DataFrame\n",
        "# Asegúrate de que 'ano' es una columna numérica\n",
        "df['ano'] = pd.to_numeric(df['ano'], errors='coerce')\n",
        "\n",
        "# Agrupa los datos por 'ano' y 'Estado Civil', y cuenta la cantidad de estudiantes\n",
        "datos_agrupados = df.groupby(['ano', 'Estado Civil'])['Código Estudiante Banner'].count().reset_index()\n",
        "\n",
        "# Crea la gráfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.lineplot(data=datos_agrupados, x='ano', y='Código Estudiante Banner', hue='Estado Civil', palette=colores_oscuros, lw=2)\n",
        "plt.title('Cantidad de estudiantes por año y Estado Civil', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Año', fontsize=16)\n",
        "plt.ylabel('Cantidad de Estudiantes', fontsize=16)\n",
        "plt.legend(title='Estado Civil', title_fontsize='14', fontsize='12', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
        "plt.xticks(rotation=45)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S57sGGhv91pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configura el estilo de Seaborn\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Define una paleta de colores más oscura\n",
        "colores_oscuros = sns.color_palette(\"dark\", as_cmap=True)\n",
        "\n",
        "# Suponiendo que df es tu DataFrame\n",
        "# Asegúrate de que 'ano' es una columna numérica\n",
        "df['ano'] = pd.to_numeric(df['ano'], errors='coerce')\n",
        "\n",
        "# Agrupa los datos por 'ano' y 'Estrato Social', y cuenta la cantidad de estudiantes\n",
        "datos_agrupados = df.groupby(['ano', 'Estrato Social'])['Código Estudiante Banner'].count().reset_index()\n",
        "\n",
        "# Crea la gráfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.lineplot(data=datos_agrupados, x='ano', y='Código Estudiante Banner', hue='Estrato Social', palette=colores_oscuros, lw=2)\n",
        "plt.title('Cantidad de estudiantes por año y Estrato Social', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Año', fontsize=16)\n",
        "plt.ylabel('Cantidad de Estudiantes', fontsize=16)\n",
        "plt.legend(title='Estrato Social', title_fontsize='14', fontsize='12', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "ax.get_xaxis().get_major_formatter().set_useOffset(False)\n",
        "plt.xticks(rotation=45)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ma871E26_FCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configura el estilo de Seaborn\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Define una paleta de colores más oscura\n",
        "colores_oscuros = sns.color_palette(\"dark\", as_cmap=True)\n",
        "\n",
        "# Suponiendo que df es tu DataFrame\n",
        "# Asegúrate de que 'Edad' es una columna numérica\n",
        "df['Edad'] = pd.to_numeric(df['Edad'], errors='coerce')\n",
        "\n",
        "# Crea la gráfica\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = sns.boxplot(data=df, x='ano', y='Edad', palette=colores_oscuros)\n",
        "plt.title('Distribución de la Edad por Año', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Año', fontsize=16)\n",
        "plt.ylabel('Edad', fontsize=16)\n",
        "ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dPHa582n0FjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa las bibliotecas necesarias\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Suponiendo que df es tu DataFrame\n",
        "# Asegúrate de que 'Edad' y 'Estrato Social' son columnas numéricas\n",
        "df['Edad'] = pd.to_numeric(df['Edad'], errors='coerce')\n",
        "df['Estrato Social'] = pd.to_numeric(df['Estrato Social'], errors='coerce')\n",
        "\n",
        "# Agrupa los datos por 'Edad' y 'Estrato Social', y cuenta la cantidad de estudiantes\n",
        "datos_agrupados = df.groupby(['Edad', 'Estrato Social']).size().reset_index(name='Cantidad de Estudiantes')\n",
        "\n",
        "# Calcula la matriz de correlación\n",
        "corr = datos_agrupados.corr()\n",
        "\n",
        "# Configura el tamaño y el estilo de la figura\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.set_style('white')\n",
        "\n",
        "# Crea una máscara para la parte superior del triángulo (opcional)\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Configura la paleta de colores\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "# Dibuja el mapa de calor\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt=\".2f\")\n",
        "\n",
        "# Configura el título y las etiquetas\n",
        "plt.title('Correlation Plot', fontsize=20)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Muestra el gráfico\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Wa_Lm9D0FgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5ojU0HovCi6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "revisar = df[df.semestre=='01']\n",
        "revisar['Cuatrimestre población'].value_counts(normalize=True)\n"
      ],
      "metadata": {
        "id": "SpHSCmfkbfJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17 a 21 anos"
      ],
      "metadata": {
        "id": "_JUEfZREFG0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "kZ40bMfiFIBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Estrato Social'].value_counts(normalize=True)*100"
      ],
      "metadata": {
        "id": "mTsACRVcTwX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Cuatrimestre población'].unique()"
      ],
      "metadata": {
        "id": "Cu-LQFRjUdUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Preparación de los datos"
      ],
      "metadata": {
        "id": "tG2qgIQ8X6A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cuatrimestre semestre 1 = 1 y 2 y semestre 1\n",
        "\n",
        "#eliminar cursos libres."
      ],
      "metadata": {
        "id": "QSbiAopST-9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "7zhNFFwmUwT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Cuatrimestre población'].unique()"
      ],
      "metadata": {
        "id": "0WX0KTEcZsNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df.Jornada!= '***No Usar**  No aplica']\n",
        "df = df[df['Cuatrimestre población']!='Curso libre']\n",
        "df_final = df[['ano', 'Cuatrimestre población', 'Sede', 'Rectoría/Vicerrectoría', 'Fecha Nacimiento', 'Descripción Metodología', 'Código Estudiante Banner']]\n",
        "df_final"
      ],
      "metadata": {
        "id": "MJ49jqA4alcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "df_final['semestre'] = np.where(\n",
        "    df_final['Cuatrimestre población'].isin(['cuatrimestre 1', 'Semestre 1', 'cuatrimestre 2']),\n",
        "    1,\n",
        "    2\n",
        ")\n",
        "df_final = df_final.rename(columns={\n",
        "    'Rectoría/Vicerrectoría': 'Rectoria',\n",
        "    'Descripción Metodología': 'Modalidad'\n",
        "})\n",
        "\n",
        "# Asegúrate de que 'Fecha Nacimiento' esté en formato datetime\n",
        "df_final['Fecha Nacimiento'] = pd.to_datetime(df_final['Fecha Nacimiento'])\n",
        "\n",
        "# Paso 2: Calcular la edad\n",
        "# Asumiendo que la columna 'ano' contiene el año actual o un año específico\n",
        "df_final['edad'] = df_final['ano'] - df_final['Fecha Nacimiento'].dt.year\n",
        "\n",
        "\n",
        "df_final['objetivo'] = df_final['edad'].apply(lambda x: 1 if 17 <= x <= 21 else 0)\n",
        "\n",
        "\n",
        "# Paso 3: Eliminar la columna 'Fecha Nacimiento'\n",
        "df_final = df_final.drop(columns=['Fecha Nacimiento', 'Cuatrimestre población', 'edad'])\n",
        "\n",
        "# Mostrar el DataFrame actualizado\n",
        "df_final"
      ],
      "metadata": {
        "id": "SCcsu1oeeFoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_sede(row):\n",
        "    if row['Modalidad'] == \"Distancia Tradicional\" and row['Sede'] == \"CERES CIUDAD BOLIVAR\":\n",
        "        return \"COA CD BOLIVAR DIST\"\n",
        "    elif row['Modalidad'] == \"Presencial\" and row['Sede'] == \"CERES CIUDAD BOLIVAR\":\n",
        "        return \"COA CD BOLIVAR PRES\"\n",
        "    elif row['Modalidad'] == \"Distancia Tradicional\" and row['Sede'] == \"CO BOSA\":\n",
        "        return \"COA BOSA DIST\"\n",
        "    elif row['Modalidad'] == \"Presencial\" and row['Sede'] == \"CO BOSA\":\n",
        "        return \"COA BOSA PRES\"\n",
        "    elif row['Modalidad'] == \"Distancia Tradicional\" and row['Sede'] == \"CO KENNEDY\":\n",
        "        return \"COA KENNEDY DIST\"\n",
        "    elif row['Modalidad'] == \"Presencial\" and row['Sede'] == \"CO KENNEDY\":\n",
        "        return \"COA KENNEDY PRES\"\n",
        "    elif row['Modalidad'] == \"Distancia Tradicional\" and row['Sede'] == \"CO RAFAEL URIBE\":\n",
        "        return \"COA RAFAEL URIBE DIST\"\n",
        "    elif row['Modalidad'] == \"Presencial\" and row['Sede'] == \"CO RAFAEL URIBE\":\n",
        "        return \"COA RAFAEL URIBE PRES\"\n",
        "    elif row['Modalidad'] == \"CO CANDELARIA\":\n",
        "        return \"COA ENGATIVA PRES\"\n",
        "    elif row['Modalidad'] == \"CO TUNAL\":\n",
        "        return \"COA ENGATIVA PRES\"\n",
        "    elif row['Modalidad'] == \"Ceres Valle de Aburrá\":\n",
        "        return \"COA BELLO\"\n",
        "    elif row['Modalidad'] == \"CERES PRADO CALI\":\n",
        "        return \"COA CALI\"\n",
        "    else:\n",
        "        return row['Modalidad']\n",
        "\n",
        "df_final['Modalidad'] = df_final.apply(update_sede, axis=1)\n",
        "df_final['Sede'] = df_final['Sede'].replace('^COA|^CO', 'CU', regex=True)\n",
        "\n",
        "\n",
        "def update_rectoria(row):\n",
        "    if row['Rectoria'] == \"Vicerrectoría Regional Bogotá Sur\":\n",
        "        return \"Rectoría UNIMINUTO Bogotá\"\n",
        "    else:\n",
        "        return row['Rectoria']\n",
        "\n",
        "df_final['Rectoria'] = df_final.apply(update_rectoria, axis=1)"
      ],
      "metadata": {
        "id": "DMyr18TWsgik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.columns"
      ],
      "metadata": {
        "id": "TaPfTAWio08X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Asumiendo que ya tienes df_final\n",
        "#df_agrupado = df_final.groupby(['ano', 'semestre', 'Rectoria', 'Sede', 'Modalidad', 'objetivo']).size().reset_index(name='y')\n",
        "df_agrupado = df_final.groupby(['ano', 'semestre', 'Rectoria', 'Modalidad', 'objetivo']).size().reset_index(name='y')\n",
        "\n",
        "\n",
        "# Identificador unico por timeseries.\n",
        "#df_agrupado['unique_id'] = df_agrupado.groupby(['Rectoria', 'Sede', 'Modalidad', 'objetivo']).ngroup() + 1\n",
        "df_agrupado['unique_id'] = df_agrupado.groupby(['Rectoria', 'Modalidad', 'objetivo']).ngroup() + 1\n",
        "\n",
        "def grouped_data(df):\n",
        "    if df.empty:\n",
        "        print(\"DataFrame is empty.\")\n",
        "        return df\n",
        "    if \"unique_id\" not in df.columns:\n",
        "        print(\"'unique_id' column does not exist in the DataFrame.\")\n",
        "        return df\n",
        "\n",
        "    grouped = df.groupby([\"unique_id\"], as_index=False).agg(\n",
        "        {\"y\": [\"mean\", \"std\"]}\n",
        "    )\n",
        "    grouped.columns = grouped.columns.map(\"_\".join)\n",
        "    grouped = grouped.reset_index(drop=True).rename(\n",
        "        columns={\"unique_id_\": \"unique_id\"}\n",
        "    )\n",
        "    grouped[\"y_std\"] = grouped[\"y_std\"].fillna(0)\n",
        "    grouped[\"upper_limit\"] = grouped[\"y_mean\"] + 3 * grouped[\"y_std\"]\n",
        "    grouped[\"lower_limit\"] = grouped[\"y_mean\"] - 3 * grouped[\"y_std\"]\n",
        "\n",
        "    df = df.merge(grouped, how=\"left\", on=\"unique_id\")\n",
        "    df.loc[\n",
        "        (df[\"y\"] > df[\"upper_limit\"])\n",
        "        | (df[\"y\"] < df[\"lower_limit\"]),\n",
        "        \"y\",\n",
        "    ] = df[\"y_mean\"]\n",
        "    df = df[df.y > 0]\n",
        "\n",
        "    return df\n",
        "\n",
        "df_agrupado = grouped_data(df_agrupado)\n",
        "\n",
        "# Asumiendo que df_transformed es tu DataFrame y que 'ano' y 'semestre' son las columnas de interés\n",
        "df_agrupado['time_index'] = (df_agrupado['ano'] - df_agrupado['ano'].min()) * 2 + df_agrupado['semestre']\n",
        "\n",
        "# Convertir 'time_index' a entero si es necesario (por si 'semestre' es una columna de tipo string)\n",
        "df_agrupado['time_index'] = df_agrupado['time_index'].astype(int)\n",
        "\n",
        "\n",
        "df_agrupado"
      ],
      "metadata": {
        "id": "8YLMIbbrpV20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = df_agrupado[['ano', 'semestre', 'time_index', 'Rectoria']]\n",
        "unique_rows = a.drop_duplicates()\n",
        "unique_rows"
      ],
      "metadata": {
        "id": "f-kxLU6-rLP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine 'ano' and 'semestre' into a single period column\n",
        "unique_rows['period'] = unique_rows['ano'].astype(str) + '-' + unique_rows['semestre'].astype(str)\n",
        "\n",
        "# Grouping by 'Rectoria' and performing the aggregation\n",
        "grouped = unique_rows.groupby('Rectoria').agg({\n",
        "    'period': ['min', 'max'],\n",
        "    'time_index': 'count'\n",
        "}).reset_index()\n",
        "\n",
        "# Flattening the column names and renaming for clarity\n",
        "grouped.columns = ['_'.join(col).strip() if col[1] else col[0] for col in grouped.columns.values]\n",
        "grouped.rename(columns={\n",
        "    'period_min': 'Start_Period',\n",
        "    'period_max': 'End_Period',\n",
        "    'time_index_count': 'Num_Semesters'\n",
        "}, inplace=True)\n",
        "\n",
        "grouped[['Rectoria', 'Num_Semesters', 'Start_Period', 'End_Period']]\n"
      ],
      "metadata": {
        "id": "bNDXqevTtGCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agrupado.unique_id.nunique()"
      ],
      "metadata": {
        "id": "RhdcNRYzy639"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agrupado.to_excel('por_rectoria.xlsx')"
      ],
      "metadata": {
        "id": "036cPoCnoZwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modelado"
      ],
      "metadata": {
        "id": "MkQAi1MrF9kn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se realizaran una validación de diferentes metodos y X, para validar la mejor estrategia."
      ],
      "metadata": {
        "id": "Q98WdgyGpThv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.1. Matriz original division en train y test."
      ],
      "metadata": {
        "id": "FLG3y7Kipa8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creando el dataset de entrenamiento\n",
        "train = df_agrupado.loc[df_agrupado['time_index'] <= 20]\n",
        "\n",
        "# Creando el dataset de validación\n",
        "validation = df_agrupado.loc[(df_agrupado['time_index'] >= 21) & (df_agrupado['time_index'] <= 25)]\n",
        "\n",
        "# Creando el dataset de prueba\n",
        "test = df_agrupado.loc[(df_agrupado['time_index'] >= 26) & (df_agrupado['time_index'] <= 30)]"
      ],
      "metadata": {
        "id": "3MTb6iUfCBoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_time_index_train = train['time_index'].nunique()\n",
        "num_time_index_validation = validation['time_index'].nunique()\n",
        "num_time_index_test = test['time_index'].nunique()\n",
        "\n",
        "print(f\"Number of unique time indices in train: {num_time_index_train}\")\n",
        "print(f\"Number of unique time indices in validation: {num_time_index_validation}\")\n",
        "print(f\"Number of unique time indices in test: {num_time_index_test}\")"
      ],
      "metadata": {
        "id": "gho0kgc6wSBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.2 crear dos datasets distintos, uno para cada semestre para predecir anual, siendo dos modelos que predigan."
      ],
      "metadata": {
        "id": "hbPVKfWzp_Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_agrupado is already loaded as per your provided code\n",
        "\n",
        "# Splitting the dataset into two datasets based on the semester\n",
        "df_semester_1 = df_agrupado[df_agrupado['semestre'] == 1].copy()  # Using copy() to avoid warnings\n",
        "df_semester_2 = df_agrupado[df_agrupado['semestre'] == 2].copy()\n",
        "\n",
        "# Function to reset time_index for continuity\n",
        "def reset_time_index(df):\n",
        "    sorted_unique_time = sorted(df['time_index'].unique())\n",
        "    mapping = {time: idx+1 for idx, time in enumerate(sorted_unique_time)}\n",
        "    df['time_index'] = df['time_index'].map(mapping)\n",
        "    return df\n",
        "\n",
        "def split_data_exact(df):\n",
        "    unique_time_indices = sorted(df['time_index'].unique())\n",
        "\n",
        "    train_cutoff = unique_time_indices[8] if len(unique_time_indices) > 9 else unique_time_indices[-1]\n",
        "    validation_cutoff = unique_time_indices[11] if len(unique_time_indices) > 12 else unique_time_indices[-1]\n",
        "\n",
        "    train = df[df['time_index'] <= train_cutoff]\n",
        "    validation = df[(df['time_index'] > train_cutoff) & (df['time_index'] <= validation_cutoff)]\n",
        "    test = df[df['time_index'] > validation_cutoff]\n",
        "\n",
        "    return train, validation, test\n",
        "\n",
        "# Resetting time_index for continuity\n",
        "df_semester_1 = reset_time_index(df_semester_1)\n",
        "df_semester_2 = reset_time_index(df_semester_2)\n",
        "\n",
        "# Getting train, validation, and test datasets for both semesters using the exact split function\n",
        "train_1, validation_1, test_1 = split_data_exact(df_semester_1)\n",
        "train_2, validation_2, test_2 = split_data_exact(df_semester_2)\n",
        "\n",
        "# Counting unique time_index values for each dataset\n",
        "num_time_index_train_1 = train_1['time_index'].nunique()\n",
        "num_time_index_validation_1 = validation_1['time_index'].nunique()\n",
        "num_time_index_test_1 = test_1['time_index'].nunique()\n",
        "\n",
        "num_time_index_train_2 = train_2['time_index'].nunique()\n",
        "num_time_index_validation_2 = validation_2['time_index'].nunique()\n",
        "num_time_index_test_2 = test_2['time_index'].nunique()\n",
        "\n",
        "print(f\"Number of unique time indices in train_1: {num_time_index_train_1}\")\n",
        "print(f\"Number of unique time indices in validation_1: {num_time_index_validation_1}\")\n",
        "print(f\"Number of unique time indices in test_1: {num_time_index_test_1}\")\n",
        "\n",
        "print(f\"Number of unique time indices in train_2: {num_time_index_train_2}\")\n",
        "print(f\"Number of unique time indices in validation_2: {num_time_index_validation_2}\")\n",
        "print(f\"Number of unique time indices in test_2: {num_time_index_test_2}\")"
      ],
      "metadata": {
        "id": "MLU2uQDZt0EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.3 agregar al df original variables predictoras de passport."
      ],
      "metadata": {
        "id": "fn-_9rEZ0QJL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9M9wedYwISA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clases de predicción."
      ],
      "metadata": {
        "id": "CZyCXe-8eKXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsforecast.core import StatsForecast\n",
        "from statsforecast.models import (\n",
        "    AutoARIMA, AutoETS, AutoCES, AutoTheta,\n",
        "    SeasonalNaive, ADIDA, CrostonClassic, IMAPA, TSB\n",
        ")\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import dump\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class Autoregresive:\n",
        "    def __init__(self, train, test, df, season_length=2, semestres_predecir=5):\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.df = df\n",
        "        self.season_length = season_length\n",
        "        self.semestres_predecir = semestres_predecir\n",
        "        self.Y_hat_df = None\n",
        "        self.tiempo = None\n",
        "        self.models = None\n",
        "        self.merged_df = None\n",
        "        self.validacion = None\n",
        "        self.merged_df = pd.DataFrame()\n",
        "\n",
        "    def format_data(self):\n",
        "        self.train['ds'] = pd.to_datetime(self.train['ano'].astype(str) + '-' + (self.train['semestre'] * 6).astype(str) + '-01')\n",
        "        self.test['ds'] = pd.to_datetime(self.test['ano'].astype(str) + '-' + (self.test['semestre'] * 6).astype(str) + '-01')\n",
        "        self.df['ds'] = pd.to_datetime(self.df['ano'].astype(str) + '-' + (self.df['semestre'] * 6).astype(str) + '-01')\n",
        "\n",
        "        train_SeriePronosticar = self.train[[\"ds\", \"unique_id\", \"y\"]].copy().fillna(0)\n",
        "        test_SeriePronosticar = self.test[[\"ds\", \"unique_id\"]].copy().fillna(0)\n",
        "        SeriePronosticar = self.df[[\"ds\", \"unique_id\", \"y\"]].copy().fillna(0)\n",
        "\n",
        "        return train_SeriePronosticar, test_SeriePronosticar, SeriePronosticar\n",
        "\n",
        "    def setup_models_and_forecast(self, train_data):\n",
        "        horizon = self.meses_predecir\n",
        "        self.models = [\n",
        "            AutoARIMA(season_length=self.season_length),\n",
        "            AutoETS(season_length=self.season_length),\n",
        "            AutoCES(season_length=self.season_length),\n",
        "            AutoTheta(season_length=self.season_length),\n",
        "            SeasonalNaive(season_length=self.season_length),\n",
        "            ADIDA(),\n",
        "            CrostonClassic(),\n",
        "            IMAPA(),\n",
        "            TSB(alpha_d=0.2, alpha_p=0.2),\n",
        "        ]\n",
        "\n",
        "        model = StatsForecast(\n",
        "            df=train_data,\n",
        "            models=self.models,\n",
        "            freq=\"6M\",\n",
        "            n_jobs=-1,\n",
        "            fallback_model=SeasonalNaive(season_length=1),\n",
        "        )\n",
        "\n",
        "        self.Y_hat_df = model.forecast(horizon)\n",
        "\n",
        "    def post_process(self):\n",
        "        self.tiempo = self.df[[\"ds\", \"time_idx\"]].drop_duplicates()\n",
        "        self.Y_hat_df = self.Y_hat_df.reset_index()\n",
        "        self.Y_hat_df[\"ds\"] = pd.to_datetime(self.Y_hat_df[\"ds\"]) + pd.Timedelta(days=1)\n",
        "        self.Y_hat_df = self.Y_hat_df.merge(\n",
        "            self.tiempo[[\"ds\", \"time_idx\"]], left_on=[\"ds\"], right_on=[\"ds\"], how=\"left\"\n",
        "        )\n",
        "\n",
        "        self.Y_hat_df[\"key\"] = (\n",
        "            self.Y_hat_df[\"time_idx\"].astype(str)\n",
        "            + \"_\"\n",
        "            + self.Y_hat_df[\"unique_id\"].astype(str)\n",
        "        )\n",
        "        self.Y_hat_df = self.Y_hat_df.rename(\n",
        "            columns={\n",
        "                \"AutoARIMA\": \"y_AutoARIMA\",\n",
        "                \"AutoETS\": \"y_AutoETS\",\n",
        "                \"CES\": \"y_AutoCES\",\n",
        "                \"AutoTheta\": \"y_AutoTheta\",\n",
        "                \"SeasonalNaive\": \"y_SeasonalNaive\",\n",
        "                \"ADIDA\": \"y_ADIDA\",\n",
        "                \"CrostonClassic\": \"y_CrostonClassic\",\n",
        "                \"IMAPA\": \"y_IMAPA\",\n",
        "                \"TSB\": \"y_TSB\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "        columns = [\n",
        "            \"y_AutoARIMA\",\n",
        "            \"y_AutoETS\",\n",
        "            \"y_AutoCES\",\n",
        "            \"y_AutoTheta\",\n",
        "            \"y_SeasonalNaive\",\n",
        "            \"y_ADIDA\",\n",
        "            \"y_CrostonClassic\",\n",
        "            \"y_IMAPA\",\n",
        "            \"y_TSB\",\n",
        "        ]\n",
        "\n",
        "        for col in columns:\n",
        "            self.Y_hat_df[col] = np.nan_to_num(self.Y_hat_df[col])\n",
        "            self.Y_hat_df[col] = np.where(self.Y_hat_df[col] < 0, 0, self.Y_hat_df[col])\n",
        "\n",
        "    def merge_with_validacion(self):\n",
        "        self.validacion = self.test.copy()  #\n",
        "        self.validacion[\"key\"] = (\n",
        "            self.validacion[\"time_idx\"].astype(str)\n",
        "            + \"_\"\n",
        "            + self.validacion[\"unique_id\"].astype(str)\n",
        "        )\n",
        "\n",
        "        self.merged_df = self.validacion.merge(\n",
        "            self.Y_hat_df[\n",
        "                [\n",
        "                    \"y_AutoARIMA\",\n",
        "                    \"y_AutoETS\",\n",
        "                    \"y_AutoCES\",\n",
        "                    \"y_AutoTheta\",\n",
        "                    \"y_SeasonalNaive\",\n",
        "                    \"y_ADIDA\",\n",
        "                    \"y_CrostonClassic\",\n",
        "                    \"y_IMAPA\",\n",
        "                    \"y_TSB\",\n",
        "                    \"key\",\n",
        "                ]\n",
        "            ],\n",
        "            left_on=[\"key\"],\n",
        "            right_on=[\"key\"],\n",
        "            how=\"left\",\n",
        "        )\n",
        "        self.merged_df = self.merged_df.drop(\"key\", axis=1)\n",
        "\n",
        "    def run_workflow(self):\n",
        "        train_data, test_data, SeriePronosticar = self.format_data()\n",
        "        self.setup_models_and_forecast(train_data)\n",
        "        self.post_process()\n",
        "        self.merge_with_validacion()\n",
        "\n",
        "        return self.merged_df\n",
        "\n",
        "    def save_model(self):\n",
        "       today = datetime.now().strftime('%Y-%m-%d')\n",
        "       filename = f\"Modelos/Autoregresive_{today}.joblib\"\n",
        "       dump(self.models, filename)\n",
        "       print(f\"Model saved as {filename}\")"
      ],
      "metadata": {
        "id": "fkxbMmK3eJ4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = train.append(validation)\n",
        "\n",
        "#from models import Autoregresive, GradientBoostingModels\n",
        "\n",
        "# se realiza la predicción de los datos reales menos los ultimos 'meses_predecir' seleccionado, tanto en el ETL como parametros de entrada.\n",
        "# validacion, es una copia del test con las predicciones para elegir el mejor modelo\n",
        "forecasting = Autoregresive(train, validation, df, season_length=2, semestres_predecir=5)\n",
        "validacion = forecasting.run_workflow()\n",
        "\n",
        "train_f = train.append(validation)\n",
        "test_f = test.cppy()\n",
        "df_f = train_f.append(test_f)\n",
        "\n",
        "forecasting_2 = Autoregresive(\n",
        "    train_f, test_f, df_f, season_length=2, semestres_predecir=5)\n",
        "\n",
        "\n",
        "teste = forecasting_2.run_workflow()\n",
        "forecasting.save_model()  # Aquí guardamos el modelo\n"
      ],
      "metadata": {
        "id": "kx3gD7KNjBEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils import column_or_1d\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from lightgbm import LGBMRegressor\n",
        "import os\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class MyLabelEncoder(LabelEncoder):\n",
        "    def fit(self, y):\n",
        "        y = column_or_1d(y, warn=True)\n",
        "        self.classes_ = np.append(pd.Series(y).unique(), [\"Unseen\"])\n",
        "        return self\n",
        "\n",
        "    def transform(self, y):\n",
        "        classes = pd.Series(y).unique()\n",
        "        new_classes = set(classes) - set(self.classes_)\n",
        "        for c in new_classes:\n",
        "            y[y == c] = \"Unseen\"\n",
        "        return super().transform(y)\n",
        "\n",
        "    def fit_transform(self, y):\n",
        "        return self.fit(y).transform(y)\n",
        "\n",
        "\n",
        "class GradientBoostingModels:\n",
        "    def __init__(self, train, validation, test, validacion, teste):\n",
        "        self.adapt_dates(train)\n",
        "        self.adapt_dates(validation)\n",
        "        self.adapt_dates(test)\n",
        "\n",
        "        self.train = train\n",
        "        self.validation = validation\n",
        "        self.test = test\n",
        "        self.validacion = validacion\n",
        "        self.teste = teste\n",
        "        self.models = {\n",
        "            \"lightgbm\": lgb.LGBMRegressor(),\n",
        "            \"xgboost\": xgb.XGBRegressor(),\n",
        "            \"catboost\": CatBoostRegressor(),\n",
        "        }\n",
        "        self.encoders = {}\n",
        "\n",
        "    def preprocess_data(self):\n",
        "       for col in self.train.columns:\n",
        "           if self.train[col].dtype == \"object\":\n",
        "               encoder = MyLabelEncoder()\n",
        "               self.train[col] = encoder.fit_transform(self.train[col])\n",
        "               self.encoders[col] = encoder\n",
        "               if col in self.validation.columns:\n",
        "                   self.validation[col] = encoder.transform(self.validation[col])\n",
        "               if col in self.test.columns:\n",
        "                   self.test[col] = encoder.transform(self.test[col])\n",
        "\n",
        "\n",
        "    def format_data(self):\n",
        "        self.train['ds'] = pd.to_datetime(self.train['ano'].astype(str) + '-' + (self.train['semestre'] * 6).astype(str) + '-01')\n",
        "        self.validation['ds'] = pd.to_datetime(self.validation['ano'].astype(str) + '-' + (self.validation['semestre'] * 6).astype(str) + '-01')\n",
        "        self.test['ds'] = pd.to_datetime(self.test['ano'].astype(str) + '-' + (self.test['semestre'] * 6).astype(str) + '-01')\n",
        "\n",
        "\n",
        "    def feature_selection(self):\n",
        "        X_train, y_train = self.train.drop(\"y\", axis=1), self.train[\"y\"]\n",
        "\n",
        "        # Using RandomForestRegressor for feature selection\n",
        "        model = RandomForestRegressor()\n",
        "        rfecv = RFECV(\n",
        "            estimator=model, step=1, cv=KFold(10), scoring=\"neg_mean_squared_error\"\n",
        "        )\n",
        "        rfecv.fit(X_train, y_train)\n",
        "\n",
        "        selected_features = X_train.columns[rfecv.support_]\n",
        "\n",
        "        # Keep only selected features\n",
        "        self.train = self.train[[\"y\"] + list(selected_features)]\n",
        "        self.validation = (\n",
        "            self.validation[[\"y\"] + list(selected_features)]\n",
        "            if \"y\" in self.validation.columns\n",
        "            else self.validation[list(selected_features)]\n",
        "        )\n",
        "        self.test = self.test[list(selected_features)]\n",
        "\n",
        "    def train_models(self):\n",
        "        self.preprocess_data()\n",
        "        self.feature_selection()\n",
        "        X_train, y_train = self.train.drop(\"y\", axis=1), self.train[\"y\"]\n",
        "        X_val, y_val = self.validation.drop(\"y\", axis=1), self.validation[\"y\"]\n",
        "\n",
        "        for model in self.models.values():\n",
        "            model.fit(\n",
        "                X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10\n",
        "            )\n",
        "\n",
        "    def predict(self):\n",
        "        self.train_models()\n",
        "        predictions_validation = {}\n",
        "        predictions_test = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            X_validation = (\n",
        "                self.validation.drop(\"y\", axis=1)\n",
        "                if \"y\" in self.validation.columns\n",
        "                else self.validation\n",
        "            )\n",
        "            predictions_validation[name] = model.predict(X_validation)\n",
        "            predictions_test[name] = model.predict(self.test)\n",
        "\n",
        "        return predictions_validation, predictions_test\n",
        "\n",
        "    def merge_predictions(self, predictions_validation, predictions_test):\n",
        "        self.validacion = self.validacion.copy()\n",
        "        self.validacion[\"key\"] = (\n",
        "            self.validacion[\"time_idx\"].astype(str)\n",
        "            + \"_\"\n",
        "            + self.validacion[\"unique_id\"].astype(str)\n",
        "        )\n",
        "\n",
        "        for name, preds in predictions_validation.items():\n",
        "            temp_df_validacion = pd.DataFrame(\n",
        "                {\"key\": self.validacion[\"key\"], f\"y_{name}\": preds}\n",
        "            )\n",
        "            self.validacion = self.validacion.merge(\n",
        "                temp_df_validacion, on=\"key\", how=\"left\"\n",
        "            )\n",
        "\n",
        "        self.teste = self.teste.copy()\n",
        "        self.teste[\"key\"] = (\n",
        "            self.teste[\"time_idx\"].astype(str)\n",
        "            + \"_\"\n",
        "            + self.teste[\"unique_id\"].astype(str)\n",
        "        )\n",
        "\n",
        "        for name, preds in predictions_test.items():\n",
        "            temp_df_teste = pd.DataFrame({\"key\": self.teste[\"key\"], f\"y_{name}\": preds})\n",
        "            self.teste = self.teste.merge(temp_df_teste, on=\"key\", how=\"left\")\n",
        "\n",
        "        # list of columns to check for negative and NaN values\n",
        "        columns = [\n",
        "            \"y_AutoARIMA\",\n",
        "            \"y_AutoETS\",\n",
        "            \"y_AutoCES\",\n",
        "            \"y_AutoTheta\",\n",
        "            \"y_SeasonalNaive\",\n",
        "            \"y_ADIDA\",\n",
        "            \"y_CrostonClassic\",\n",
        "            \"y_IMAPA\",\n",
        "            \"y_TSB\",\n",
        "            \"y_lightgbm\",\n",
        "            \"y_xgboost\",\n",
        "            \"y_catboost\",\n",
        "        ]\n",
        "\n",
        "        # replace negative values and NaNs with 0 in the validation and test data\n",
        "        for col in columns:\n",
        "            if col in self.validacion.columns:\n",
        "                self.validacion[col] = self.validacion[col].apply(\n",
        "                    lambda x: 0 if x < 0 or np.isnan(x) else x\n",
        "                )\n",
        "            if col in self.teste.columns:\n",
        "                self.teste[col] = self.teste[col].apply(\n",
        "                    lambda x: 0 if x < 0 or np.isnan(x) else x\n",
        "                )\n",
        "\n",
        "        self.validacion = self.validacion.drop(\"key\", axis=1)\n",
        "        self.teste = self.teste.drop(\"key\", axis=1)\n",
        "\n",
        "        return self.validacion, self.teste\n",
        "\n",
        "    def pipeline(self):\n",
        "        self.format_data()\n",
        "        predictions_validation, predictions_test = self.predict()\n",
        "        validation_data_final, test_data_final = self.merge_predictions(\n",
        "            predictions_validation, predictions_test\n",
        "        )\n",
        "        return validation_data_final, test_data_final\n",
        "\n",
        "    def save_models(self):\n",
        "        # Obtener la fecha actual\n",
        "        current_date = datetime.now().strftime(\"%Y%m%d\")\n",
        "        # Guardar cada modelo con su nombre y la fecha actual\n",
        "        for name, model in self.models.items():\n",
        "            filename = f\"Modelos/{name}_{current_date}.pkl\"\n",
        "            data_to_save = {\n",
        "                'model': model,\n",
        "                'encoders': self.encoders\n",
        "            }\n",
        "            joblib.dump(data_to_save, filename)"
      ],
      "metadata": {
        "id": "XTK-geTij_Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# se crean 3 modelos de regresión con boosting, toma como datos de entrada train, los datos menos los 'meses_predecir' seleccionado,\n",
        "# validación es el periodo dado por esos últimos n meses, test_f datos de inferencia, y para validación y teste, son  matrices con inferencias del modelo anterior.\n",
        "fore = GradientBoostingModels(train, validation, test_f, validacion, teste)\n",
        "validacion, teste = fore.pipeline()\n",
        "fore.save_models()   # Guardar los modelos\n",
        "validacion"
      ],
      "metadata": {
        "id": "acpnj1AmnId8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}